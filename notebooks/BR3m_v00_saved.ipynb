{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S7vKulYNoqld",
        "outputId": "485b5275-566a-4922-a309-ccc936dfd8eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "torch version before installing wheels: 2.9.0+cu126\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/aiohappyeyeballs-2.6.1-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/aiosignal-1.4.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/attrs-25.4.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/certifi-2025.11.12-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/fsspec-2025.10.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/idna-3.11-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/jinja2-3.1.6-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/pyparsing-3.2.5-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/requests-2.32.5-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_cluster-1.6.3-cp312-cp312-linux_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_geometric-2.7.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_scatter-2.1.2-cp312-cp312-linux_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_sparse-0.6.18-cp312-cp312-linux_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_spline_conv-1.2.2-cp312-cp312-linux_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/tqdm-4.67.1-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/typing_extensions-4.15.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/urllib3-2.5.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "aiohappyeyeballs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "aiohttp is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "aiosignal is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "attrs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "certifi is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "charset-normalizer is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "frozenlist is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "idna is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "jinja2 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "markupsafe is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "multidict is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "propcache is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "pyparsing is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "scipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "tqdm is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "typing-extensions is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "urllib3 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "xxhash is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "yarl is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, psutil, numpy, fsspec, requests, torch-sparse, torch-cluster, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.10.0 numpy-2.3.5 psutil-7.1.3 requests-2.32.5 torch-cluster-1.6.3 torch-geometric-2.7.0 torch-scatter-2.1.2 torch-sparse-0.6.18 torch-spline-conv-1.2.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ca7c2b2714fa4f7db62a72c15d2ce6ce",
              "pip_warning": {
                "packages": [
                  "numpy",
                  "psutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4079828407.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install {WHEEL_DIR}/*.whl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_geometric:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_home_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_home_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_mps_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_xpu_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0misinstance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_torch_instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/isinstance.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWITH_PT20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Register polyfill functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpolyfills\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;31m# usort: skip # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/loader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;34m\"tensor\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0;31m POLYFILLED_MODULES: tuple[\"ModuleType\", ...] = tuple(\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{submodule}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolyfills\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPOLYFILLED_MODULE_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/loader.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m POLYFILLED_MODULES: tuple[\"ModuleType\", ...] = tuple(\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{submodule}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolyfills\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPOLYFILLED_MODULE_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m )\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/_collections.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0m_collections\u001b[0m  \u001b[0;31m# type: ignore[import-not-found]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0msubstitute_in_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     def _count_elements(\n\u001b[1;32m     22\u001b[0m         \u001b[0mmapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(traceable_fn)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Duplicate polyfilled object {original_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mrule_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVariableTracker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_torch_obj_rule_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moriginal_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrule_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36mget_torch_obj_rule_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2994\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\".py#\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2996\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2997\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m                 \u001b[0mtorch_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_module_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36mload_object\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   3030\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Invalid obj name {name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3032\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_obj_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3033\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap_if_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36m_load_obj_from_str\u001b[0;34m(fully_qualified_name)\u001b[0m\n\u001b[1;32m   3014\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_obj_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfully_qualified_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3015\u001b[0m     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfully_qualified_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3016\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_onnx_program\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mONNXProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from ._internal.torchscript_exporter import (  # Deprecated members that are excluded from __all__\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_symbolic_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/ops/_symbolic_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m @torch.library.custom_op(\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;34m\"onnx_symbolic::_symbolic\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mmutates_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomOpDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# Check that schema's alias annotations match those of `mutates_args`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, namespace, name, schema, fn, tags)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_library_allowing_overwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_to_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disabled_kernel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_used_triton_kernels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36m_register_to_dispatcher\u001b[0;34m(self, tags)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0mautograd_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_autograd_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opoverload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograd_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Autograd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_keyset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m in \u001b[0;36mmake_autograd_impl\u001b[0;34m(op, info)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mhas_kwarg_only_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_kwarg_only_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mMetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mkeyset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatchKeySet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/dataclasses.py\u001b[0m in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;31m# We're called as @dataclass without parens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/dataclasses.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \"\"\"\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         return _process_class(cls, init, repr, eq, order, unsafe_hash,\n\u001b[1;32m   1266\u001b[0m                               \u001b[0mfrozen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# CELL 0 - Mount Drive and install PyG + deps from saved wheels\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "WHEEL_DIR = \"/content/drive/MyDrive/PyG_wheels_torch29_cu126\"\n",
        "\n",
        "import torch\n",
        "print(\"torch version before installing wheels:\", torch.__version__)\n",
        "\n",
        "# ðŸ‘‰ This assumes Colab still has torch==2.9.0+cu126.\n",
        "# If torch version changes in the future, you'll want a new wheel set.\n",
        "!pip install {WHEEL_DIR}/*.whl\n",
        "\n",
        "import torch_geometric\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torch_geometric:\", torch_geometric.__version__)\n",
        "\n",
        "from torch_geometric.datasets import QM9\n",
        "print(\"QM9 import OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-4NLJc3p8nl",
        "outputId": "249d254b-db09-4608-b002-99364e1c4b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'QM9_project'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 40 (delta 7), reused 36 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (40/40), 24.75 KiB | 6.19 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "/content/QM9_project\n",
            "Already up to date.\n",
            "bootstrapwheels.py  how2git.txt\t\t\t README.md\n",
            "data\t\t    models\t\t\t requirements.txt\n",
            "env\t\t    newColabenv.py\t\t setup_pyg_wheels.py\n",
            "eval_qm9_legacy.py  orig_train_qm9_baseline.py\t train_qm9_baseline.py\n",
            "fastboot.sh\t    qm9_project_bootstrap.ipynb\n"
          ]
        }
      ],
      "source": [
        "# CELL 1 - Clone or refresh QM9_project repo\n",
        "\n",
        "%cd /content\n",
        "!test -d QM9_project || git clone https://github.com/zeugirdoR/QM9_project.git\n",
        "%cd QM9_project\n",
        "!git pull\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9g3w3uLNiwE",
        "outputId": "96dfc263-f78a-45a9-cbe2-586e5d9af660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting models/v20_agaa_micro.py\n"
          ]
        }
      ],
      "source": [
        "# CELL - V20-AGAA-MICRO MODEL (PyG-friendly + motor_strength)\n",
        "%%writefile models/v20_agaa_micro.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "\n",
        "def safe_norm(x, dim=-1, eps=1e-6):\n",
        "    return torch.sqrt(torch.sum(x ** 2, dim=dim) + eps)\n",
        "\n",
        "\n",
        "def coulomb_potential(pos, eps=1e-6):\n",
        "    # pos: (B, N, 3)\n",
        "    delta = pos.unsqueeze(2) - pos.unsqueeze(1)   # (B, N, N, 3)\n",
        "    dist  = safe_norm(delta, dim=-1)             # (B, N, N)\n",
        "    inv_dist = torch.where(dist > 1e-4, 1.0 / dist, torch.zeros_like(dist))\n",
        "    return inv_dist.unsqueeze(-1)                # (B, N, N, 1)\n",
        "\n",
        "\n",
        "class GaussianRBF(nn.Module):\n",
        "    def __init__(self, n_rbf: int = 20, cutoff: float = 5.0):\n",
        "        super().__init__()\n",
        "        self.n_rbf = n_rbf\n",
        "        centers = torch.linspace(0.0, cutoff, n_rbf)\n",
        "        widths  = torch.full((n_rbf,), cutoff / n_rbf)\n",
        "        self.register_buffer(\"centers\", centers)\n",
        "        self.register_buffer(\"widths\", widths)\n",
        "\n",
        "    def forward(self, dist: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        dist: (B, N, N)\n",
        "        returns: (B, N, N, n_rbf)\n",
        "        \"\"\"\n",
        "        diff = dist.unsqueeze(-1) - self.centers.view(1, 1, 1, -1)\n",
        "        widths = torch.clamp(self.widths, min=1e-3)\n",
        "        return torch.exp(- (diff ** 2) / (2.0 * widths.view(1, 1, 1, -1) ** 2))\n",
        "\n",
        "\n",
        "class MotorAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head 'motor' attention in 6D screw space, gated by Coulomb + RBF.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int, n_rbf: int = 20):\n",
        "        super().__init__()\n",
        "        self.n_heads   = n_heads\n",
        "        self.dim_motor = 6\n",
        "\n",
        "        self.q_screw = nn.Linear(d_model, n_heads * self.dim_motor, bias=False)\n",
        "        self.k_screw = nn.Linear(d_model, n_heads * self.dim_motor, bias=False)\n",
        "\n",
        "        self.coulomb_proj = nn.Linear(1, n_heads, bias=False)\n",
        "        self.rbf_gate  = nn.Linear(n_rbf, n_heads, bias=True)\n",
        "        self.rbf_bias  = nn.Linear(n_rbf, n_heads, bias=False)\n",
        "\n",
        "        self.v_proj   = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.metric = nn.Parameter(torch.tensor([1., 1., 1., 1., 1., 1.]))\n",
        "        self.scale  = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,         # (B, N, d_model)\n",
        "        rbf_feat: torch.Tensor,  # (B, N, N, n_rbf)\n",
        "        pos: torch.Tensor,       # (B, N, 3)\n",
        "        mask: torch.Tensor = None,   # (B, N) bool or 0/1\n",
        "        motor_strength: float = 1.0, # 0 = motors silent\n",
        "    ):\n",
        "        B, N, _ = x.shape\n",
        "        x_f32 = x.float()\n",
        "\n",
        "        # screw-space Q,K\n",
        "        q = self.q_screw(x_f32).view(B, N, self.n_heads, self.dim_motor).permute(0, 2, 1, 3)\n",
        "        k = self.k_screw(x_f32).view(B, N, self.n_heads, self.dim_motor).permute(0, 2, 1, 3)\n",
        "\n",
        "        metric = self.metric.view(1, 1, 1, self.dim_motor)\n",
        "        q_m = q * metric\n",
        "        k_m = k * metric\n",
        "\n",
        "        motor_score = torch.einsum(\"bhnc,bhmc->bhnm\", q_m, k_m)\n",
        "        motor_score = motor_score * self.scale / math.sqrt(self.dim_motor)\n",
        "\n",
        "        coul = coulomb_potential(pos).float()        # (B, N, N, 1)\n",
        "        coul = self.coulomb_proj(coul)               # (B, N, N, H)\n",
        "\n",
        "        rbf = rbf_feat.float()\n",
        "        gate = self.rbf_gate(rbf) + self.rbf_bias(rbf)   # (B, N, N, H)\n",
        "\n",
        "        geo = coul + gate                            # (B, N, N, H)\n",
        "        geo = geo.permute(0, 3, 1, 2)                # (B, H, N, N)\n",
        "\n",
        "        # curriculum knob\n",
        "        scores = geo + motor_strength * motor_score  # (B, H, N, N)\n",
        "\n",
        "        # motor activity scalar (for volume penalty + monitoring)\n",
        "        sig_motor = (motor_score * gate.permute(0, 3, 1, 2)).abs().mean()\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_mask = mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,N)\n",
        "            scores = scores.masked_fill(attn_mask == 0, float(\"-inf\"))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)         # (B, H, N, N)\n",
        "\n",
        "        v = self.v_proj(x_f32)                       # (B, N, d_model)\n",
        "        d_head = x_f32.size(-1) // self.n_heads\n",
        "        v = v.view(B, N, self.n_heads, d_head).permute(0, 2, 1, 3)\n",
        "\n",
        "        out = torch.matmul(attn, v)                  # (B, H, N, d_head)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(B, N, -1)\n",
        "\n",
        "        out = self.out_proj(out).to(x.dtype)\n",
        "        return out, sig_motor\n",
        "\n",
        "\n",
        "class V20_Block_Motor(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, n_rbf: int = 20):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.attn  = MotorAttention(d_model, n_heads, n_rbf=n_rbf)\n",
        "        self.ffn   = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(4 * d_model, d_model),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, rbf_feat, pos, mask=None, motor_strength: float = 1.0):\n",
        "        h = self.norm1(x)\n",
        "        attn_out, sig = self.attn(h, rbf_feat, pos, mask, motor_strength=motor_strength)\n",
        "        x = x + attn_out\n",
        "\n",
        "        h = self.norm2(x)\n",
        "        x = x + self.ffn(h)\n",
        "        return x, sig\n",
        "\n",
        "\n",
        "class V20_AGAA_Motor(nn.Module):\n",
        "    \"\"\"\n",
        "    PyG-friendly variant:\n",
        "      - takes batched node tensors (z, pos, batch_idx)\n",
        "      - internally pads to (B, N, Â·) with to_dense_batch\n",
        "      - returns (pred, sig_motor) with pred shape (B, 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers=7, d_model=192, n_heads=16, max_z=100, n_rbf=20):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_rbf   = n_rbf\n",
        "\n",
        "        self.rbf   = GaussianRBF(n_rbf=n_rbf, cutoff=5.0)\n",
        "        self.emb_z = nn.Embedding(max_z, d_model)\n",
        "        self.emb_geo  = nn.Linear(4, d_model)\n",
        "        self.emb_fuse = nn.Linear(2 * d_model, d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [V20_Block_Motor(d_model, n_heads, n_rbf=n_rbf) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.norm_final = nn.LayerNorm(d_model)\n",
        "        self.head       = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, z, pos, batch_idx=None, motor_strength: float = 1.0):\n",
        "        \"\"\"\n",
        "        z         : (M,)      atomic numbers for all nodes\n",
        "        pos       : (M, 3)    positions for all nodes\n",
        "        batch_idx : (M,)      graph index for each node (PyG's batch)\n",
        "        motor_strength: curriculum knob\n",
        "        \"\"\"\n",
        "        if batch_idx is None:\n",
        "            # Single graph case: treat as batch size 1\n",
        "            pos_dense = pos.unsqueeze(0)          # (1, N, 3)\n",
        "            z_dense   = z.unsqueeze(0)            # (1, N)\n",
        "            mask = torch.ones_like(z_dense, dtype=torch.bool, device=z_dense.device)\n",
        "        else:\n",
        "            pos_dense, mask = to_dense_batch(pos, batch_idx)  # (B, N, 3), (B, N)\n",
        "            z_dense,  _     = to_dense_batch(z,   batch_idx)  # (B, N)\n",
        "\n",
        "        mask_float = mask.float()                 # (B, N)\n",
        "        z_clamped = z_dense.clamp(min=0).long()   # (B, N)\n",
        "\n",
        "        # Atom type emb\n",
        "        h_z = self.emb_z(z_clamped)              # (B, N, d_model)\n",
        "\n",
        "        # Pairwise distances in each molecule\n",
        "        delta = pos_dense.unsqueeze(2) - pos_dense.unsqueeze(1)  # (B, N, N, 3)\n",
        "        dist  = safe_norm(delta, dim=-1)                         # (B, N, N)\n",
        "\n",
        "        rbf_feat = self.rbf(dist)                               # (B, N, N, n_rbf)\n",
        "\n",
        "        # Per-atom geometric summaries (mean/min/max/std of local distances)\n",
        "        neigh_mask = mask_float.unsqueeze(1) * mask_float.unsqueeze(2)  # (B, N, N)\n",
        "        dist_masked = dist * neigh_mask\n",
        "        valid_counts = neigh_mask.sum(dim=-1).clamp(min=1.0)            # (B, N)\n",
        "\n",
        "        mean_d = dist_masked.sum(dim=-1) / valid_counts                 # (B, N)\n",
        "        min_d  = torch.where(\n",
        "            neigh_mask.bool(),\n",
        "            dist,\n",
        "            torch.full_like(dist, 1e6)\n",
        "        ).min(dim=-1).values                                           # (B, N)\n",
        "        max_d  = (dist * neigh_mask).max(dim=-1).values                # (B, N)\n",
        "        std_d  = torch.sqrt(\n",
        "            torch.clamp((dist_masked ** 2).sum(dim=-1) / valid_counts - mean_d ** 2, min=0.0)\n",
        "        )                                                              # (B, N)\n",
        "\n",
        "        geo_feat = torch.stack([mean_d, min_d, max_d, std_d], dim=-1)  # (B, N, 4)\n",
        "        h_geo = self.emb_geo(geo_feat)\n",
        "\n",
        "        h = torch.cat([h_z, h_geo], dim=-1)                            # (B, N, 2*d_model)\n",
        "        h = self.emb_fuse(h)                                           # (B, N, d_model)\n",
        "\n",
        "        sigs = []\n",
        "        for layer in self.layers:\n",
        "            h, sig = layer(h, rbf_feat, pos_dense, mask, motor_strength=motor_strength)\n",
        "            sigs.append(sig)\n",
        "\n",
        "        h = self.norm_final(h)\n",
        "\n",
        "        # mask-based pooling\n",
        "        mask_f = mask_float.unsqueeze(-1)                               # (B, N, 1)\n",
        "        h_pool = (h * mask_f).sum(dim=1) / mask_f.sum(dim=1).clamp(min=1.0)  # (B, d_model)\n",
        "\n",
        "        pred = self.head(h_pool)                                       # (B, 1)\n",
        "        sig_motor = torch.stack(sigs).mean()\n",
        "        return pred, sig_motor\n",
        "\n",
        "\n",
        "def build_v20_agaa_micro():\n",
        "    return V20_AGAA_Motor(num_layers=7, d_model=192, n_heads=16, max_z=100, n_rbf=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4faejZK_LrMg",
        "outputId": "7172fe98-05ab-47b2-d8bf-dc692bac5166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 2950034\n"
          ]
        }
      ],
      "source": [
        "# CELL - Reload model and re-check params\n",
        "import importlib\n",
        "import models.v20_agaa_micro as v20\n",
        "\n",
        "importlib.reload(v20)\n",
        "\n",
        "model = v20.V20_AGAA_Motor(\n",
        "    num_layers=7,\n",
        "    d_model=192,\n",
        "    n_heads=16,\n",
        "    max_z=100,\n",
        "    n_rbf=20,\n",
        ").to(\"cuda\")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Trainable params:\", total_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8tQyi7nQpd9"
      },
      "outputs": [],
      "source": [
        "# CELL - Curriculum training for V20-AGAA-Motor (sparse â†’ dense handled inside model)\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ðŸ‘‰ Choose which QM9 property to learn.\n",
        "#    Use the same index you used in the warm test.\n",
        "TARGET_IDX = 12   # adjust if you trained on a different QM9 column\n",
        "\n",
        "\n",
        "def _extract_target(batch):\n",
        "    \"\"\"\n",
        "    Extract a 1D target vector (batch of scalars) from batch.y.\n",
        "\n",
        "    Handles:\n",
        "      - (B, 19): full QM9 target vector per graph  -> pick column TARGET_IDX\n",
        "      - (B, 1) : already scalar                    -> squeeze to (B,)\n",
        "      - (B,)   : already scalar                    -> keep as is\n",
        "    \"\"\"\n",
        "    y = batch.y\n",
        "    if y.dim() == 2:\n",
        "        if y.size(1) == 19:\n",
        "            y = y[:, TARGET_IDX]\n",
        "        elif y.size(1) == 1:\n",
        "            y = y.squeeze(-1)\n",
        "        else:\n",
        "            y = y[:, 0]  # fallback\n",
        "    else:\n",
        "        y = y.view(-1)\n",
        "    return y\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    motor_strength: float,\n",
        "    lambda_motor_reg: float = 0.0,\n",
        "    scale_to_meV: float = 1.0,\n",
        "):\n",
        "    model.train()\n",
        "    total_mae = 0.0\n",
        "    total_mot = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        z         = batch.z          # (total_nodes,)\n",
        "        pos       = batch.pos        # (total_nodes, 3)\n",
        "        batch_idx = batch.batch      # (total_nodes,)\n",
        "        y         = _extract_target(batch)  # (B,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Let V20_AGAA_Motor do to_dense_batch internally:\n",
        "        pred, sig_motor = model(z, pos, batch_idx, motor_strength=motor_strength)\n",
        "\n",
        "        # Make prediction 1D to match y\n",
        "        pred = pred.view(-1)\n",
        "\n",
        "        if pred.numel() != y.numel():\n",
        "            raise RuntimeError(\n",
        "                f\"Shape mismatch in train_one_epoch: pred {pred.shape}, y {y.shape}\"\n",
        "            )\n",
        "\n",
        "        loss_data = F.l1_loss(pred, y)\n",
        "        # approximate \"volume\" penalty via motor activity\n",
        "        loss = loss_data + lambda_motor_reg * motor_strength * sig_motor\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mae_meV = loss_data.item() * scale_to_meV\n",
        "        total_mae += mae_meV\n",
        "        total_mot += sig_motor.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_mae / max(n_batches, 1), total_mot / max(n_batches, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    device,\n",
        "    motor_strength: float,\n",
        "    scale_to_meV: float = 1.0,\n",
        "):\n",
        "    model.eval()\n",
        "    total_mae = 0.0\n",
        "    total_mot = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        z         = batch.z\n",
        "        pos       = batch.pos\n",
        "        batch_idx = batch.batch\n",
        "        y         = _extract_target(batch)  # (B,)\n",
        "\n",
        "        pred, sig_motor = model(z, pos, batch_idx, motor_strength=motor_strength)\n",
        "        pred = pred.view(-1)\n",
        "\n",
        "        if pred.numel() != y.numel():\n",
        "            raise RuntimeError(\n",
        "                f\"Shape mismatch in eval_epoch: pred {pred.shape}, y {y.shape}\"\n",
        "            )\n",
        "\n",
        "        loss_data = F.l1_loss(pred, y)\n",
        "        mae_meV = loss_data.item() * scale_to_meV\n",
        "\n",
        "        total_mae += mae_meV\n",
        "        total_mot += sig_motor.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_mae / max(n_batches, 1), total_mot / max(n_batches, 1)\n",
        "\n",
        "\n",
        "def run_curriculum_training(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    device,\n",
        "    epochs=200,\n",
        "    motor_unlock_meV=10.0,      # when train MAE < this, start ramp\n",
        "    motor_ramp_epochs=50,\n",
        "    motor_max_strength=1.0,\n",
        "    lambda_motor_reg=1e-3,      # volume penalty weight\n",
        "    scale_to_meV=1.0,\n",
        "):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_epoch = -1\n",
        "    curriculum_unlocked = False\n",
        "    unlock_epoch = None\n",
        "\n",
        "    print(\"Ep | motors | train | val   | test  | ms | best\")\n",
        "    print(\"---+--------+-------+-------+-------+----+-----\")\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        # decide motor_strength for this epoch\n",
        "        if not curriculum_unlocked:\n",
        "            motor_strength = 0.0\n",
        "        else:\n",
        "            t = max(0, ep - unlock_epoch)\n",
        "            motor_strength = motor_max_strength * min(1.0, t / max(motor_ramp_epochs, 1))\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        train_mae, train_mot = train_one_epoch(\n",
        "            model, train_loader, optimizer, device,\n",
        "            motor_strength=motor_strength,\n",
        "            lambda_motor_reg=lambda_motor_reg,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        val_mae,   val_mot   = eval_epoch(\n",
        "            model, val_loader, device,\n",
        "            motor_strength=motor_strength,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        test_mae,  test_mot  = eval_epoch(\n",
        "            model, test_loader, device,\n",
        "            motor_strength=motor_strength,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        dt = time.time() - t0  # you can add a 'sec' column later if you want\n",
        "\n",
        "        # unlock motors when scalar-only fit is good enough\n",
        "        if (not curriculum_unlocked) and (train_mae <= motor_unlock_meV):\n",
        "            curriculum_unlocked = True\n",
        "            unlock_epoch = ep\n",
        "\n",
        "        # best mark based on val\n",
        "        is_best = val_mae < best_val\n",
        "        if is_best:\n",
        "            best_val = val_mae\n",
        "            best_epoch = ep\n",
        "        star = \"*\" if is_best else \" \"\n",
        "\n",
        "        # motors column = val_mot (avg motor activity on val)\n",
        "        # ms column      = motor_strength for this epoch\n",
        "        print(\n",
        "            f\"{ep:3d} | {val_mot:6.3f} | \"\n",
        "            f\"{train_mae:5.1f} | {val_mae:5.1f} | {test_mae:5.1f} | \"\n",
        "            f\"{motor_strength:3.2f} | {star}\"\n",
        "        )\n",
        "\n",
        "    print(f\"\\nBest val: {best_val:.3f} meV at epoch {best_epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKPu3GgXRUdV",
        "outputId": "f6f37082-3f1e-4db7-c7dc-e55a626e12d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep | motors | train | val   | test  | ms | best\n",
            "---+--------+-------+-------+-------+----+-----\n",
            "  1 |  0.050 |  44.2 |  19.0 |   6.4 | 0.00 | *\n",
            "  2 |  0.048 |   6.6 |   3.2 |   2.0 | 0.00 | *\n",
            "  3 |  0.599 |   2.8 |   1.7 |   1.6 | 0.02 | *\n",
            "  4 |  4.252 |   1.9 |   1.8 |   1.3 | 0.03 |  \n",
            "  5 |  6.033 |   1.3 |   1.0 |   1.2 | 0.05 | *\n",
            "  6 |  6.346 |   1.1 |   0.7 |   1.0 | 0.07 | *\n",
            "  7 |  7.229 |   0.9 |   0.8 |   1.1 | 0.08 |  \n",
            "  8 |  6.963 |   0.8 |   0.8 |   0.9 | 0.10 |  \n",
            "  9 |  6.117 |   1.3 |   1.4 |   1.2 | 0.12 |  \n",
            " 10 |  6.138 |   0.9 |   0.7 |   1.3 | 0.13 | *\n",
            " 11 |  5.784 |   0.6 |   0.5 |   1.1 | 0.15 | *\n",
            " 12 |  5.525 |   0.7 |   0.5 |   1.1 | 0.17 | *\n",
            " 13 |  5.350 |   0.6 |   0.5 |   1.1 | 0.18 | *\n",
            " 14 |  5.610 |   0.8 |   0.5 |   0.9 | 0.20 | *\n",
            " 15 |  5.500 |   0.5 |   0.5 |   0.9 | 0.22 |  \n",
            " 16 |  5.603 |   1.3 |   0.6 |   0.8 | 0.23 |  \n",
            " 17 |  5.250 |   0.6 |   0.4 |   1.0 | 0.25 | *\n",
            " 18 |  4.780 |   0.4 |   0.4 |   0.8 | 0.27 |  \n",
            " 19 |  4.389 |   0.4 |   0.4 |   0.8 | 0.28 | *\n",
            " 20 |  4.421 |   0.7 |   0.6 |   1.2 | 0.30 |  \n",
            " 21 |  4.411 |   0.5 |   0.5 |   1.0 | 0.32 |  \n",
            " 22 |  2.915 |   0.6 |   1.1 |   1.1 | 0.33 |  \n",
            " 23 |  3.412 |   1.1 |   0.5 |   1.1 | 0.35 |  \n",
            " 24 |  3.907 |   0.7 |   0.4 |   0.7 | 0.37 |  \n",
            " 25 |  3.549 |   0.4 |   0.4 |   0.8 | 0.38 |  \n",
            " 26 |  3.227 |   0.4 |   0.6 |   1.3 | 0.40 |  \n",
            " 27 |  2.906 |   0.3 |   0.4 |   1.0 | 0.42 |  \n",
            " 28 |  2.678 |   0.5 |   0.3 |   0.9 | 0.43 | *\n",
            " 29 |  2.432 |   0.3 |   0.4 |   1.0 | 0.45 |  \n",
            " 30 |  2.007 |   1.1 |   0.5 |   1.1 | 0.47 |  \n",
            " 31 |  1.308 |   0.9 |   0.9 |   1.0 | 0.48 |  \n",
            " 32 |  1.451 |   0.8 |   0.5 |   1.0 | 0.50 |  \n",
            " 33 |  1.460 |   0.4 |   0.3 |   0.9 | 0.52 |  \n",
            " 34 |  1.756 |   0.6 |   0.3 |   0.7 | 0.53 | *\n",
            " 35 |  1.767 |   0.4 |   0.3 |   0.8 | 0.55 | *\n",
            " 36 |  1.820 |   0.4 |   0.4 |   0.8 | 0.57 |  \n",
            " 37 |  1.842 |   0.3 |   0.4 |   1.0 | 0.58 |  \n",
            " 38 |  2.411 |   0.6 |   0.4 |   1.0 | 0.60 |  \n",
            " 39 |  2.179 |   0.3 |   0.3 |   0.8 | 0.62 |  \n",
            " 40 |  1.985 |   0.3 |   0.3 |   0.6 | 0.63 | *\n",
            " 41 |  1.817 |   0.3 |   0.3 |   0.9 | 0.65 |  \n",
            " 42 |  1.640 |   0.3 |   0.2 |   0.7 | 0.67 | *\n",
            " 43 |  1.514 |   0.3 |   0.3 |   0.6 | 0.68 |  \n",
            " 44 |  1.392 |   0.4 |   0.2 |   0.7 | 0.70 | *\n",
            " 45 |  1.234 |   0.2 |   0.2 |   0.7 | 0.72 |  \n",
            " 46 |  1.121 |   0.2 |   0.3 |   0.6 | 0.73 |  \n",
            " 47 |  1.031 |   0.4 |   0.2 |   0.6 | 0.75 | *\n",
            " 48 |  0.948 |   0.2 |   0.2 |   0.6 | 0.77 | *\n",
            " 49 |  0.879 |   0.2 |   0.2 |   0.6 | 0.78 | *\n",
            " 50 |  0.908 |   0.5 |   0.3 |   0.7 | 0.80 |  \n",
            " 51 |  0.795 |   0.2 |   0.2 |   0.6 | 0.82 | *\n",
            " 52 |  0.751 |   0.2 |   0.2 |   0.5 | 0.83 |  \n",
            " 53 |  0.708 |   0.2 |   0.2 |   0.5 | 0.85 |  \n",
            " 54 |  0.886 |   0.8 |   0.5 |   1.0 | 0.87 |  \n",
            " 55 |  0.763 |   0.4 |   0.3 |   0.9 | 0.88 |  \n",
            " 56 |  0.673 |   0.4 |   0.3 |   0.7 | 0.90 |  \n",
            " 57 |  0.639 |   0.2 |   0.3 |   0.8 | 0.92 |  \n",
            " 58 |  0.612 |   0.2 |   0.2 |   0.5 | 0.93 |  \n",
            " 59 |  0.578 |   0.2 |   0.3 |   1.1 | 0.95 |  \n",
            " 60 |  0.540 |   0.9 |   0.7 |   0.7 | 0.97 |  \n",
            " 61 |  0.513 |   0.3 |   0.2 |   0.5 | 0.98 |  \n",
            " 62 |  0.490 |   0.2 |   0.3 |   0.4 | 1.00 |  \n",
            " 63 |  0.458 |   0.2 |   0.2 |   0.4 | 1.00 |  \n",
            " 64 |  0.430 |   0.2 |   0.2 |   0.4 | 1.00 |  \n",
            " 65 |  0.422 |   0.2 |   0.2 |   0.5 | 1.00 |  \n",
            " 66 |  0.407 |   0.7 |   1.6 |   2.0 | 1.00 |  \n",
            " 67 |  0.434 |   0.4 |   0.2 |   0.5 | 1.00 |  \n",
            " 68 |  0.425 |   0.2 |   0.3 |   0.5 | 1.00 |  \n",
            " 69 |  0.417 |   0.2 |   0.3 |   0.8 | 1.00 |  \n",
            " 70 |  0.404 |   0.2 |   0.3 |   0.7 | 1.00 |  \n",
            " 71 |  0.391 |   0.2 |   0.2 |   0.5 | 1.00 |  \n",
            " 72 |  0.393 |   0.4 |   0.3 |   0.5 | 1.00 |  \n",
            " 73 |  0.380 |   0.2 |   0.1 |   0.4 | 1.00 | *\n",
            " 74 |  0.363 |   0.2 |   0.2 |   0.5 | 1.00 |  \n",
            " 75 |  0.355 |   0.2 |   0.1 |   0.4 | 1.00 | *\n",
            " 76 |  0.346 |   0.1 |   0.2 |   0.5 | 1.00 |  \n",
            " 77 |  0.334 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            " 78 |  0.327 |   0.2 |   0.2 |   0.4 | 1.00 |  \n",
            " 79 |  0.333 |   0.2 |   0.2 |   0.5 | 1.00 |  \n",
            " 80 |  0.317 |   0.1 |   0.2 |   0.5 | 1.00 |  \n",
            " 81 |  0.318 |   0.2 |   0.1 |   0.4 | 1.00 |  \n",
            " 82 |  0.310 |   0.1 |   0.1 |   0.4 | 1.00 | *\n",
            " 83 |  0.301 |   0.1 |   0.3 |   0.5 | 1.00 |  \n",
            " 84 |  0.324 |   0.3 |   0.7 |   0.8 | 1.00 |  \n",
            " 85 |  0.311 |   0.3 |   0.2 |   0.5 | 1.00 |  \n",
            " 86 |  0.297 |   0.1 |   0.1 |   0.3 | 1.00 | *\n",
            " 87 |  0.295 |   0.1 |   0.1 |   0.3 | 1.00 | *\n",
            " 88 |  0.318 |   0.2 |   0.5 |   0.6 | 1.00 |  \n",
            " 89 |  0.293 |   0.1 |   0.2 |   0.3 | 1.00 |  \n",
            " 90 |  0.283 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            " 91 |  0.278 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            " 92 |  0.270 |   0.1 |   0.2 |   0.3 | 1.00 |  \n",
            " 93 |  0.294 |   0.2 |   0.3 |   0.9 | 1.00 |  \n",
            " 94 |  0.273 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            " 95 |  0.265 |   0.1 |   0.2 |   0.3 | 1.00 |  \n",
            " 96 |  0.261 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            " 97 |  0.258 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            " 98 |  0.258 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            " 99 |  0.250 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            "100 |  0.246 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            "101 |  0.241 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "102 |  0.237 |   0.1 |   0.2 |   0.3 | 1.00 |  \n",
            "103 |  0.235 |   0.1 |   0.1 |   0.3 | 1.00 | *\n",
            "104 |  0.233 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "105 |  0.231 |   0.7 |   0.3 |   0.7 | 1.00 |  \n",
            "106 |  0.238 |   0.2 |   0.2 |   0.7 | 1.00 |  \n",
            "107 |  0.238 |   0.2 |   0.2 |   0.7 | 1.00 |  \n",
            "108 |  0.237 |   0.1 |   0.1 |   0.5 | 1.00 |  \n",
            "109 |  0.231 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "110 |  0.241 |   0.5 |   0.6 |   0.9 | 1.00 |  \n",
            "111 |  0.233 |   0.3 |   0.1 |   0.3 | 1.00 |  \n",
            "112 |  0.223 |   0.1 |   0.2 |   0.2 | 1.00 |  \n",
            "113 |  0.215 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "114 |  0.210 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "115 |  0.207 |   0.1 |   0.2 |   0.2 | 1.00 |  \n",
            "116 |  0.204 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "117 |  0.212 |   0.1 |   0.4 |   0.5 | 1.00 |  \n",
            "118 |  0.213 |   0.2 |   0.3 |   0.8 | 1.00 |  \n",
            "119 |  0.207 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            "120 |  0.203 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "121 |  0.201 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            "122 |  0.201 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "123 |  0.187 |   0.1 |   1.1 |   1.4 | 1.00 |  \n",
            "124 |  0.213 |   0.6 |   0.2 |   0.4 | 1.00 |  \n",
            "125 |  0.214 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "126 |  0.209 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "127 |  0.208 |   0.1 |   0.1 |   0.3 | 1.00 | *\n",
            "128 |  0.208 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "129 |  0.203 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "130 |  0.201 |   0.1 |   0.2 |   0.3 | 1.00 |  \n",
            "131 |  0.216 |   0.4 |   0.2 |   0.3 | 1.00 |  \n",
            "132 |  0.205 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "133 |  0.198 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "134 |  0.196 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "135 |  0.191 |   0.1 |   0.2 |   0.5 | 1.00 |  \n",
            "136 |  0.188 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "137 |  0.189 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "138 |  0.185 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "139 |  0.182 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "140 |  0.180 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "141 |  0.176 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            "142 |  0.174 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "143 |  0.175 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "144 |  0.173 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "145 |  0.174 |   0.1 |   0.2 |   0.3 | 1.00 |  \n",
            "146 |  0.172 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "147 |  0.170 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "148 |  0.182 |   0.4 |   0.3 |   0.8 | 1.00 |  \n",
            "149 |  0.182 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "150 |  0.177 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "151 |  0.176 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            "152 |  0.173 |   0.1 |   0.1 |   0.3 | 1.00 | *\n",
            "153 |  0.172 |   0.1 |   0.2 |   0.3 | 1.00 |  \n",
            "154 |  0.169 |   0.1 |   0.1 |   0.3 | 1.00 | *\n",
            "155 |  0.167 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "156 |  0.168 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "157 |  0.165 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "158 |  0.164 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "159 |  0.160 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "160 |  0.162 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "161 |  0.161 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "162 |  0.160 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "163 |  0.158 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "164 |  0.155 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "165 |  0.157 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "166 |  0.154 |   0.1 |   0.1 |   0.2 | 1.00 | *\n",
            "167 |  0.190 |   0.4 |   0.5 |   1.4 | 1.00 |  \n",
            "168 |  0.181 |   0.2 |   0.3 |   1.2 | 1.00 |  \n",
            "169 |  0.179 |   0.1 |   0.3 |   1.1 | 1.00 |  \n",
            "170 |  0.175 |   0.1 |   0.2 |   0.8 | 1.00 |  \n",
            "171 |  0.174 |   0.1 |   0.2 |   0.7 | 1.00 |  \n",
            "172 |  0.179 |   0.2 |   0.2 |   0.7 | 1.00 |  \n",
            "173 |  0.174 |   0.1 |   0.3 |   1.1 | 1.00 |  \n",
            "174 |  0.173 |   0.1 |   0.2 |   0.8 | 1.00 |  \n",
            "175 |  0.172 |   0.1 |   0.3 |   1.3 | 1.00 |  \n",
            "176 |  0.182 |   0.2 |   0.3 |   1.2 | 1.00 |  \n",
            "177 |  0.169 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "178 |  0.165 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "179 |  0.161 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "180 |  0.161 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "181 |  0.158 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            "182 |  0.156 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "183 |  0.156 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "184 |  0.156 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "185 |  0.162 |   0.2 |   0.3 |   1.6 | 1.00 |  \n",
            "186 |  0.155 |   0.1 |   0.2 |   0.6 | 1.00 |  \n",
            "187 |  0.154 |   0.1 |   0.2 |   0.5 | 1.00 |  \n",
            "188 |  0.151 |   0.1 |   0.1 |   0.6 | 1.00 |  \n",
            "189 |  0.150 |   0.1 |   0.2 |   0.4 | 1.00 |  \n",
            "190 |  0.150 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "191 |  0.147 |   0.1 |   0.2 |   0.5 | 1.00 |  \n",
            "192 |  0.146 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "193 |  0.145 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "194 |  0.144 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "195 |  0.144 |   0.1 |   0.1 |   0.2 | 1.00 |  \n",
            "196 |  0.142 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "197 |  0.141 |   0.1 |   0.2 |   0.7 | 1.00 |  \n",
            "198 |  0.149 |   0.1 |   0.2 |   0.9 | 1.00 |  \n",
            "199 |  0.143 |   0.1 |   0.1 |   0.3 | 1.00 |  \n",
            "200 |  0.142 |   0.1 |   0.1 |   0.4 | 1.00 |  \n",
            "\n",
            "Best val: 0.077 meV at epoch 166\n"
          ]
        }
      ],
      "source": [
        "# CELL - Launch curriculum training\n",
        "\n",
        "run_curriculum_training(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    device,\n",
        "    epochs=200,\n",
        "    motor_unlock_meV=10.0,   # when train drops below ~10 meV\n",
        "    motor_ramp_epochs=60,    # slowly turn on motors over 60 epochs\n",
        "    motor_max_strength=1.0,\n",
        "    lambda_motor_reg=1e-3,   # approximate volume penalty\n",
        "    scale_to_meV=1.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RYZ0udWDcpY7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "825ddba8-7543-4dd2-968b-90f97d10cfc3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2820250744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotor_strength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0m_extract_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ],
      "source": [
        "batch = next(iter(test_loader)).to(device)\n",
        "with torch.no_grad():\n",
        "    pred, _ = model(batch.z, batch.pos, batch.batch, motor_strength=0.0)\n",
        "    pred = pred.view(-1)\n",
        "    y    = _extract_target(batch)\n",
        "    mae_eV  = (pred - y).abs().mean().item()\n",
        "    mae_meV = mae_eV * 1000\n",
        "print(\"Raw MAE:\", mae_eV, \"eV   =   \", mae_meV, \"meV\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL - Snapshot current RB3m curriculum run (config + model + env + git)\n",
        "\n",
        "import os, json, time, sys, subprocess\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "    pyg_version = torch_geometric.__version__\n",
        "except Exception:\n",
        "    pyg_version = None\n",
        "\n",
        "# ---- 1) Create a unique run directory on Drive ----\n",
        "RUN_ROOT = \"/content/drive/MyDrive/GAHEAD_runs\"\n",
        "run_name = f\"RB3m_curriculum_U0_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
        "RUN_DIR  = os.path.join(RUN_ROOT, run_name)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"ðŸ“ Snapshot directory:\", RUN_DIR)\n",
        "\n",
        "# ---- 2) Save model weights ----\n",
        "ckpt_path = os.path.join(RUN_DIR, \"RB3m_curriculum_ep24.pt\")\n",
        "torch.save(model.state_dict(), ckpt_path)\n",
        "print(\"âœ… Saved model state_dict to:\", ckpt_path)\n",
        "\n",
        "# ---- 3) Try to grab basic optimizer info (if optimizer exists) ----\n",
        "opt_info = None\n",
        "if \"optimizer\" in globals():\n",
        "    opt = optimizer\n",
        "    try:\n",
        "        opt_info = {\n",
        "            \"type\": opt.__class__.__name__,\n",
        "            \"lr\": opt.param_groups[0].get(\"lr\", None),\n",
        "            \"weight_decay\": opt.param_groups[0].get(\"weight_decay\", None),\n",
        "            \"betas\": tuple(opt.param_groups[0].get(\"betas\", (None, None))),\n",
        "            \"eps\": opt.param_groups[0].get(\"eps\", None),\n",
        "        }\n",
        "    except Exception:\n",
        "        opt_info = {\"type\": opt.__class__.__name__}\n",
        "\n",
        "# ---- 4) Build a config dict for reproducibility ----\n",
        "# Fill in anything you know exactly from your launch cell (edit if needed).\n",
        "cfg = {\n",
        "    \"description\": \"RB3m (â‰ˆ2.95M params) geometric transformer with motor curriculum on QM9 U0\",\n",
        "    \"model\": {\n",
        "        \"name\": \"V20_AGAA_Motor\",\n",
        "        \"num_layers\": 7,\n",
        "        \"d_model\": 192,\n",
        "        \"n_heads\": 16,\n",
        "        \"max_z\": 100,\n",
        "        \"n_rbf\": 20,\n",
        "        \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"dataset\": \"QM9\",\n",
        "        \"target\": \"U0\",\n",
        "        \"target_index\": 7,\n",
        "        \"root\": \"qm9_data\",\n",
        "        \"train_size\": len(train_loader.dataset) if 'train_loader' in globals() else None,\n",
        "        \"val_size\": len(val_loader.dataset) if 'val_loader' in globals() else None,\n",
        "        \"test_size\": len(test_loader.dataset) if 'test_loader' in globals() else None,\n",
        "        \"batch_train\": train_loader.batch_size if 'train_loader' in globals() else None,\n",
        "        \"batch_eval\":  test_loader.batch_size if 'test_loader' in globals() else None,\n",
        "    },\n",
        "    \"curriculum\": {\n",
        "        \"motor_unlock_meV\": globals().get(\"MOTOR_UNLOCK_MEV\", None),\n",
        "        \"motor_ramp_epochs\": globals().get(\"MOTOR_RAMP_EPOCHS\", None),\n",
        "        \"motor_max_strength\": globals().get(\"MOTOR_MAX_STRENGTH\", None),\n",
        "        \"lambda_motor_reg\": globals().get(\"LAMBDA_MOTOR_REG\", None),\n",
        "        \"scale_to_meV\": globals().get(\"SCALE_TO_MEV\", None),\n",
        "        \"epochs_completed\": 24,  # update if you ran more\n",
        "    },\n",
        "    \"optimizer\": opt_info,\n",
        "    \"seeds\": {\n",
        "        \"SEED\": globals().get(\"SEED\", None),\n",
        "        \"PYTHONHASHSEED\": os.environ.get(\"PYTHONHASHSEED\"),\n",
        "    },\n",
        "    \"environment\": {\n",
        "        \"python\": sys.version,\n",
        "        \"torch\": torch.__version__,\n",
        "        \"torch_geometric\": pyg_version,\n",
        "        \"cwd\": os.getcwd(),\n",
        "        \"device\": str(next(model.parameters()).device),\n",
        "    },\n",
        "}\n",
        "\n",
        "cfg_path = os.path.join(RUN_DIR, \"config.json\")\n",
        "with open(cfg_path, \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "print(\"âœ… Saved config to:\", cfg_path)\n",
        "\n",
        "# ---- 5) Save metrics history if we have it in memory ----\n",
        "# If your training loop kept a list `METRICS_HISTORY`, we snapshot it;\n",
        "# otherwise we just skip politely.\n",
        "try:\n",
        "    METRICS_HISTORY  # just to see if it exists\n",
        "    mh_path = os.path.join(RUN_DIR, \"metrics_history.json\")\n",
        "    import copy\n",
        "    with open(mh_path, \"w\") as f:\n",
        "        json.dump(copy.deepcopy(METRICS_HISTORY), f, indent=2)\n",
        "    print(\"âœ… Saved METRICS_HISTORY to:\", mh_path)\n",
        "except NameError:\n",
        "    print(\"â„¹ï¸ No METRICS_HISTORY variable found; skipping metrics JSON.\")\n",
        "\n",
        "# ---- 6) Capture git commit & diff for /content/QM9_project ----\n",
        "try:\n",
        "    qm9_dir = \"/content/QM9_project\"\n",
        "    commit = subprocess.check_output(\n",
        "        [\"bash\", \"-lc\", f\"cd {qm9_dir} && git rev-parse HEAD\"]\n",
        "    ).decode().strip()\n",
        "    with open(os.path.join(RUN_DIR, \"git_commit.txt\"), \"w\") as f:\n",
        "        f.write(commit + \"\\n\")\n",
        "\n",
        "    diff = subprocess.check_output(\n",
        "        [\"bash\", \"-lc\", f\"cd {qm9_dir} && git diff\"],\n",
        "        timeout=10,\n",
        "    ).decode()\n",
        "    with open(os.path.join(RUN_DIR, \"git_diff.patch\"), \"w\") as f:\n",
        "        f.write(diff)\n",
        "\n",
        "    print(\"âœ… Saved git_commit.txt and git_diff.patch.\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ Could not capture git info:\", repr(e))\n",
        "\n",
        "print(\"\\nâœ¨ Snapshot complete. This run is now reproducible from\", RUN_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "Yhw396phXWkf",
        "outputId": "d33993e0-1d03-4de9-819e-1703b205210f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Snapshot directory: /content/drive/MyDrive/GAHEAD_runs/RB3m_curriculum_U0_20251127_033245\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-125885668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# ---- 2) Save model weights ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RB3m_curriculum_ep24.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… Saved model state_dict to:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL - Robust snapshot of RB3m curriculum run (no crash if model is missing)\n",
        "\n",
        "import os, json, time, sys, subprocess\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "    pyg_version = torch_geometric.__version__\n",
        "except Exception:\n",
        "    pyg_version = None\n",
        "\n",
        "# Reuse the directory you already saw:\n",
        "RUN_DIR = \"/content/drive/MyDrive/GAHEAD_runs/RB3m_curriculum_U0_20251127_033245\"\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"ðŸ“ Snapshot directory:\", RUN_DIR)\n",
        "\n",
        "# ---- 1) Save model weights if model is in memory ----\n",
        "if \"model\" in globals():\n",
        "    ckpt_path = os.path.join(RUN_DIR, \"RB3m_curriculum_epXX.pt\")  # rename XX if you know the epoch\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(\"âœ… Saved model.state_dict() to:\", ckpt_path)\n",
        "else:\n",
        "    print(\"âš  'model' not found in this runtime; skipping direct weight save.\")\n",
        "    print(\"   If you have a checkpoint .pt file from training, you can copy it into this folder manually.\")\n",
        "\n",
        "# ---- 2) Try to grab basic optimizer info (if present) ----\n",
        "opt_info = None\n",
        "if \"optimizer\" in globals():\n",
        "    opt = optimizer\n",
        "    try:\n",
        "        opt_info = {\n",
        "            \"type\": opt.__class__.__name__,\n",
        "            \"lr\": opt.param_groups[0].get(\"lr\", None),\n",
        "            \"weight_decay\": opt.param_groups[0].get(\"weight_decay\", None),\n",
        "            \"betas\": tuple(opt.param_groups[0].get(\"betas\", (None, None))),\n",
        "            \"eps\": opt.param_groups[0].get(\"eps\", None),\n",
        "        }\n",
        "    except Exception:\n",
        "        opt_info = {\"type\": opt.__class__.__name__}\n",
        "\n",
        "# ---- 3) Build a config dict with whatever is available ----\n",
        "def _len_or_none(obj_name):\n",
        "    return len(globals()[obj_name].dataset) if obj_name in globals() else None\n",
        "\n",
        "def _bs_or_none(obj_name):\n",
        "    return globals()[obj_name].batch_size if obj_name in globals() else None\n",
        "\n",
        "trainable_params = None\n",
        "if \"model\" in globals():\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "cfg = {\n",
        "    \"description\": \"RB3m (~2.95M params) geometric transformer with motor curriculum on QM9 U0\",\n",
        "    \"model\": {\n",
        "        \"name\": \"V20_AGAA_Motor\",\n",
        "        \"num_layers\": 7,\n",
        "        \"d_model\": 192,\n",
        "        \"n_heads\": 16,\n",
        "        \"max_z\": 100,\n",
        "        \"n_rbf\": 20,\n",
        "        \"trainable_params\": trainable_params,\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"dataset\": \"QM9\",\n",
        "        \"target\": \"U0\",\n",
        "        \"target_index\": 7,\n",
        "        \"root\": \"qm9_data\",\n",
        "        \"train_size\": _len_or_none(\"train_loader\"),\n",
        "        \"val_size\":   _len_or_none(\"val_loader\"),\n",
        "        \"test_size\":  _len_or_none(\"test_loader\"),\n",
        "        \"batch_train\": _bs_or_none(\"train_loader\"),\n",
        "        \"batch_eval\":  _bs_or_none(\"test_loader\"),\n",
        "    },\n",
        "    \"curriculum\": {\n",
        "        \"motor_unlock_meV\": globals().get(\"MOTOR_UNLOCK_MEV\", None),\n",
        "        \"motor_ramp_epochs\": globals().get(\"MOTOR_RAMP_EPOCHS\", None),\n",
        "        \"motor_max_strength\": globals().get(\"MOTOR_MAX_STRENGTH\", None),\n",
        "        \"lambda_motor_reg\": globals().get(\"LAMBDA_MOTOR_REG\", None),\n",
        "        \"scale_to_meV\": globals().get(\"SCALE_TO_MEV\", None),\n",
        "        \"epochs_completed\": 24,  # adjust if you know exact\n",
        "    },\n",
        "    \"optimizer\": opt_info,\n",
        "    \"seeds\": {\n",
        "        \"SEED\": globals().get(\"SEED\", None),\n",
        "        \"PYTHONHASHSEED\": os.environ.get(\"PYTHONHASHSEED\"),\n",
        "    },\n",
        "    \"environment\": {\n",
        "        \"python\": sys.version,\n",
        "        \"torch\": torch.__version__,\n",
        "        \"torch_geometric\": pyg_version,\n",
        "        \"cwd\": os.getcwd(),\n",
        "        \"device\": str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")),\n",
        "    },\n",
        "}\n",
        "\n",
        "cfg_path = os.path.join(RUN_DIR, \"config.json\")\n",
        "with open(cfg_path, \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "print(\"âœ… Saved config to:\", cfg_path)\n",
        "\n",
        "# ---- 4) Save metrics history if it exists ----\n",
        "if \"METRICS_HISTORY\" in globals():\n",
        "    import copy\n",
        "    mh_path = os.path.join(RUN_DIR, \"metrics_history.json\")\n",
        "    with open(mh_path, \"w\") as f:\n",
        "        json.dump(copy.deepcopy(METRICS_HISTORY), f, indent=2)\n",
        "    print(\"âœ… Saved METRICS_HISTORY to:\", mh_path)\n",
        "else:\n",
        "    print(\"â„¹ï¸ No METRICS_HISTORY in this runtime; skipping metrics JSON.\")\n",
        "\n",
        "# ---- 5) Capture git commit & diff for /content/QM9_project ----\n",
        "try:\n",
        "    qm9_dir = \"/content/QM9_project\"\n",
        "    commit = subprocess.check_output(\n",
        "        [\"bash\", \"-lc\", f\"cd {qm9_dir} && git rev-parse HEAD\"]\n",
        "    ).decode().strip()\n",
        "    with open(os.path.join(RUN_DIR, \"git_commit.txt\"), \"w\") as f:\n",
        "        f.write(commit + \"\\n\")\n",
        "\n",
        "    diff = subprocess.check_output(\n",
        "        [\"bash\", \"-lc\", f\"cd {qm9_dir} && git diff\"],\n",
        "        timeout=10,\n",
        "    ).decode()\n",
        "    with open(os.path.join(RUN_DIR, \"git_diff.patch\"), \"w\") as f:\n",
        "        f.write(diff)\n",
        "\n",
        "    print(\"âœ… Saved git_commit.txt and git_diff.patch.\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ Could not capture git info:\", repr(e))\n",
        "\n",
        "print(\"\\nâœ¨ Snapshot complete (as much as this runtime can see).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYtEZ9QJYJ09",
        "outputId": "4bee6183-7c5e-4e71-d8d6-cc7a25edc63c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Snapshot directory: /content/drive/MyDrive/GAHEAD_runs/RB3m_curriculum_U0_20251127_033245\n",
            "âš  'model' not found in this runtime; skipping direct weight save.\n",
            "   If you have a checkpoint .pt file from training, you can copy it into this folder manually.\n",
            "âœ… Saved config to: /content/drive/MyDrive/GAHEAD_runs/RB3m_curriculum_U0_20251127_033245/config.json\n",
            "â„¹ï¸ No METRICS_HISTORY in this runtime; skipping metrics JSON.\n",
            "âš ï¸ Could not capture git info: CalledProcessError(1, ['bash', '-lc', 'cd /content/QM9_project && git rev-parse HEAD'])\n",
            "\n",
            "âœ¨ Snapshot complete (as much as this runtime can see).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}