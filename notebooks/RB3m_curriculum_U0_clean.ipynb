{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltvQg-PQcFFf",
        "outputId": "13d09843-a56d-456f-88b7-7a1fb5e435c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# CELL 0 - GPU info + mount Google Drive\n",
        "\n",
        "import torch, os\n",
        "\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1 - Clone QM9_project repo and cd\n",
        "\n",
        "%cd /content\n",
        "\n",
        "if not os.path.exists(\"QM9_project\"):\n",
        "    !git clone https://github.com/zeugirdoR/QM9_project.git\n",
        "\n",
        "%cd /content/QM9_project\n",
        "print(\"CWD:\", os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbsLq2VTc7yP",
        "outputId": "be1735e4-6a8b-4959-baf1-fae612e6e656"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'QM9_project'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 48 (delta 10), reused 41 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (48/48), 42.60 KiB | 21.30 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "/content/QM9_project\n",
            "CWD: /content/QM9_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 - Install PyG (torch-scatter/sparse/cluster/spline, torch-geometric) from wheels\n",
        "\n",
        "WHEEL_DIR = \"/content/drive/MyDrive/PyG_wheels_torch29_cu126\"\n",
        "\n",
        "print(\"Using wheel dir:\", WHEEL_DIR)\n",
        "!ls -1 \"$WHEEL_DIR\"\n",
        "\n",
        "# Install all wheels in that folder\n",
        "!pip install \"$WHEEL_DIR\"/*.whl\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torch_geometric:\", torch_geometric.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7-T_KRIdAmj",
        "outputId": "3265de9b-bcc9-4cd3-a051-d340e29d5040"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using wheel dir: /content/drive/MyDrive/PyG_wheels_torch29_cu126\n",
            "aiohappyeyeballs-2.6.1-py3-none-any.whl\n",
            "aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "aiosignal-1.4.0-py3-none-any.whl\n",
            "attrs-25.4.0-py3-none-any.whl\n",
            "certifi-2025.11.12-py3-none-any.whl\n",
            "charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl\n",
            "fsspec-2025.10.0-py3-none-any.whl\n",
            "idna-3.11-py3-none-any.whl\n",
            "jinja2-3.1.6-py3-none-any.whl\n",
            "markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
            "propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl\n",
            "pyparsing-3.2.5-py3-none-any.whl\n",
            "requests-2.32.5-py3-none-any.whl\n",
            "scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
            "torch_cluster-1.6.3-cp312-cp312-linux_x86_64.whl\n",
            "torch_geometric-2.7.0-py3-none-any.whl\n",
            "torch_scatter-2.1.2-cp312-cp312-linux_x86_64.whl\n",
            "torch_sparse-0.6.18-cp312-cp312-linux_x86_64.whl\n",
            "torch_spline_conv-1.2.2-cp312-cp312-linux_x86_64.whl\n",
            "tqdm-4.67.1-py3-none-any.whl\n",
            "typing_extensions-4.15.0-py3-none-any.whl\n",
            "urllib3-2.5.0-py3-none-any.whl\n",
            "xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/aiohappyeyeballs-2.6.1-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/aiosignal-1.4.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/attrs-25.4.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/certifi-2025.11.12-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/fsspec-2025.10.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/idna-3.11-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/jinja2-3.1.6-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/pyparsing-3.2.5-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/requests-2.32.5-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_cluster-1.6.3-cp312-cp312-linux_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_geometric-2.7.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_scatter-2.1.2-cp312-cp312-linux_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_sparse-0.6.18-cp312-cp312-linux_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/torch_spline_conv-1.2.2-cp312-cp312-linux_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/tqdm-4.67.1-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/typing_extensions-4.15.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/urllib3-2.5.0-py3-none-any.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "Processing ./drive/MyDrive/PyG_wheels_torch29_cu126/yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
            "aiohappyeyeballs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "aiohttp is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "aiosignal is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "attrs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "certifi is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "charset-normalizer is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "frozenlist is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "fsspec is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "idna is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "jinja2 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "markupsafe is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "multidict is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "numpy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "propcache is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "psutil is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "pyparsing is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "requests is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "scipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "torch-cluster is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "torch-geometric is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "torch-scatter is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "torch-sparse is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "torch-spline-conv is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "tqdm is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "typing-extensions is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "urllib3 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "xxhash is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "yarl is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "torch: 2.9.0+cu126\n",
            "torch_geometric: 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 - Load QM9 dataset and build DataLoaders\n",
        "# defines: train_loader, val_loader, test_loader\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "root = \"./qm9_data\"\n",
        "target_prop = \"U0\"   # we want total internal energy at 0 K\n",
        "\n",
        "# According to PyG docs, QM9.y has 19 targets with this order:\n",
        "QM9_TARGETS = [\n",
        "    \"mu\",      # 0\n",
        "    \"alpha\",   # 1\n",
        "    \"homo\",    # 2\n",
        "    \"lumo\",    # 3\n",
        "    \"gap\",     # 4\n",
        "    \"r2\",      # 5\n",
        "    \"zpve\",    # 6\n",
        "    \"U0\",      # 7  <-- THIS is total internal energy at 0 K\n",
        "    \"U\",       # 8\n",
        "    \"H\",       # 9\n",
        "    \"G\",       # 10\n",
        "    \"Cv\",      # 11\n",
        "    \"mu_0\",    # 12 (sometimes documented as per-atom etc. depending on variant)\n",
        "    \"alpha_0\", # 13\n",
        "    \"homo_0\",  # 14\n",
        "    \"lumo_0\",  # 15\n",
        "    \"gap_0\",   # 16\n",
        "    \"r2_0\",    # 17\n",
        "    \"zpve_0\",  # 18\n",
        "]\n",
        "\n",
        "try:\n",
        "    TARGET_IDX = QM9_TARGETS.index(target_prop)\n",
        "except ValueError:\n",
        "    raise RuntimeError(f\"Property {target_prop} not found in QM9_TARGETS list\")\n",
        "\n",
        "print(f\"TARGET_IDX for {target_prop} is {TARGET_IDX}\")\n",
        "\n",
        "# Load dataset (PyG's QM9 has .y with 19 targets per graph)\n",
        "dataset = QM9(root=root)\n",
        "print(\"Total graphs:\", len(dataset))\n",
        "print(\"y shape example:\", dataset[0].y.shape)\n",
        "\n",
        "# Sanity: confirm weâ€™re really seeing 19 targets:\n",
        "if dataset[0].y.numel() != 19:\n",
        "    raise RuntimeError(f\"Unexpected QM9.y size: {dataset[0].y.numel()} (expected 19)\")\n",
        "\n",
        "# Simple train/val/test split (same as before)\n",
        "num_graphs = len(dataset)\n",
        "train_num = int(num_graphs * 0.84)\n",
        "val_num   = int(num_graphs * 0.10)\n",
        "test_num  = num_graphs - train_num - val_num\n",
        "\n",
        "train_dataset = dataset[:train_num]\n",
        "val_dataset   = dataset[train_num:train_num + val_num]\n",
        "test_dataset  = dataset[train_num + val_num:]\n",
        "\n",
        "print(f\"Train/Val/Test = {len(train_dataset)}, {len(val_dataset)}, {len(test_dataset)}\")\n",
        "\n",
        "# DataLoaders â€“ big batches to use A100\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"âœ… DataLoaders ready: train_loader, val_loader, test_loader\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI29tWCnv-Pf",
        "outputId": "3d4eb104-603b-447c-9c54-a5fea1a5d32a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "TARGET_IDX for U0 is 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
            "Extracting qm9_data/raw/qm9_v3.zip\n",
            "Processing...\n",
            "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total graphs: 130831\n",
            "y shape example: torch.Size([1, 19])\n",
            "Train/Val/Test = 109898, 13083, 7850\n",
            "âœ… DataLoaders ready: train_loader, val_loader, test_loader\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 - Helpers for U0 target, model call, and epoch loops\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ðŸ‘‰ We are DEFINITELY using U0 (idx 7 in PyG QM9)\n",
        "TARGET_IDX = 7\n",
        "\n",
        "\n",
        "def _extract_target(batch):\n",
        "    \"\"\"\n",
        "    Extract U0 from batch.y, always returning shape (B,).\n",
        "\n",
        "    QM9 via torch_geometric usually gives:\n",
        "      - batch.y: (B, 19)   (all properties, including U0)\n",
        "    We pick column TARGET_IDX (7) and flatten.\n",
        "    \"\"\"\n",
        "    y = batch.y\n",
        "    if y.dim() == 2:\n",
        "        # Full QM9 vector (B, 19) or similar\n",
        "        if y.size(1) > TARGET_IDX:\n",
        "            y = y[:, TARGET_IDX]\n",
        "        else:\n",
        "            # Fallback: first column\n",
        "            y = y[:, 0]\n",
        "    else:\n",
        "        y = y.view(-1)\n",
        "    return y\n",
        "\n",
        "\n",
        "def _call_model_with_optional_motor(model, z, pos, batch_idx, motor_strength: float):\n",
        "    \"\"\"\n",
        "    Robustly call V20_AGAA_Motor, handling both:\n",
        "      - forward(z, pos, batch_idx, motor_strength=...)\n",
        "      - forward(z, pos, batch_idx)\n",
        "    and both:\n",
        "      - returns pred\n",
        "      - returns (pred, sig_motor)\n",
        "\n",
        "    Returns:\n",
        "      pred      : tensor of predictions\n",
        "      sig_scalar: scalar (float) measuring motor activity (or 0.0 if not provided)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try the \"new\" signature with motor_strength\n",
        "        out = model(z, pos, batch_idx, motor_strength=motor_strength)\n",
        "    except TypeError:\n",
        "        # Fallback: model doesn't accept motor_strength\n",
        "        out = model(z, pos, batch_idx)\n",
        "\n",
        "    if isinstance(out, tuple) and len(out) == 2:\n",
        "        pred, sig_motor = out\n",
        "        # Make a scalar measure of motor activity (mean of whatever is returned)\n",
        "        sig_scalar = float(sig_motor.float().mean().detach().item())\n",
        "    else:\n",
        "        pred = out\n",
        "        sig_scalar = 0.0  # motors not instrumented â†’ treat as silent for now\n",
        "\n",
        "    return pred, sig_scalar\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    motor_strength: float,\n",
        "    lambda_motor_reg: float = 0.0,\n",
        "    scale_to_meV: float = 1000.0,   # eV â†’ meV\n",
        "):\n",
        "    model.train()\n",
        "    total_mae = 0.0\n",
        "    total_mot = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        z         = batch.z          # (total_nodes,)\n",
        "        pos       = batch.pos        # (total_nodes, 3)\n",
        "        batch_idx = batch.batch      # (total_nodes,)\n",
        "        y         = _extract_target(batch)  # (B,) in eV\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Call model robustly\n",
        "        pred, sig_scalar = _call_model_with_optional_motor(\n",
        "            model, z, pos, batch_idx, motor_strength\n",
        "        )\n",
        "\n",
        "        pred = pred.view(-1)  # (B,)\n",
        "\n",
        "        if pred.numel() != y.numel():\n",
        "            raise RuntimeError(\n",
        "                f\"Shape mismatch in train_one_epoch: pred {pred.shape}, y {y.shape}\"\n",
        "            )\n",
        "\n",
        "        # Data loss in eV\n",
        "        loss_data = F.l1_loss(pred, y)\n",
        "\n",
        "        # Regularization via motor activity\n",
        "        loss = loss_data + lambda_motor_reg * motor_strength * sig_scalar\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mae_meV = loss_data.item() * scale_to_meV  # log in meV\n",
        "        total_mae += mae_meV\n",
        "        total_mot += sig_scalar\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_mae / max(n_batches, 1), total_mot / max(n_batches, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    device,\n",
        "    motor_strength: float,\n",
        "    scale_to_meV: float = 1000.0,\n",
        "):\n",
        "    model.eval()\n",
        "    total_mae = 0.0\n",
        "    total_mot = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        z         = batch.z\n",
        "        pos       = batch.pos\n",
        "        batch_idx = batch.batch\n",
        "        y         = _extract_target(batch)  # (B,) in eV\n",
        "\n",
        "        pred, sig_scalar = _call_model_with_optional_motor(\n",
        "            model, z, pos, batch_idx, motor_strength\n",
        "        )\n",
        "        pred = pred.view(-1)\n",
        "\n",
        "        if pred.numel() != y.numel():\n",
        "            raise RuntimeError(\n",
        "                f\"Shape mismatch in eval_epoch: pred {pred.shape}, y {y.shape}\"\n",
        "            )\n",
        "\n",
        "        loss_data = F.l1_loss(pred, y)\n",
        "        mae_meV = loss_data.item() * scale_to_meV\n",
        "\n",
        "        total_mae += mae_meV\n",
        "        total_mot += sig_scalar\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_mae / max(n_batches, 1), total_mot / max(n_batches, 1)\n"
      ],
      "metadata": {
        "id": "rlTFznP1tDCX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5 - Curriculum training with full saving & reproducibility\n",
        "\n",
        "# CELL - Clean curriculum training for U0 with V20_AGAA_Motor\n",
        "\n",
        "import os, time, json, random, subprocess\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "# ----- 1. Target selection: U0 at index 7 in PyG QM9 -----\n",
        "TARGET_IDX = 7  # U0 (eV in PyG)\n",
        "\n",
        "\n",
        "def _extract_target(batch):\n",
        "    \"\"\"\n",
        "    Extract a 1D target vector (batch of scalars) from batch.y.\n",
        "\n",
        "    For QM9: y has shape (B, 19), and U0 is at column 7 (index 7) in eV.\n",
        "    \"\"\"\n",
        "    y = batch.y\n",
        "    if y.dim() == 2:\n",
        "        if y.size(1) > TARGET_IDX:\n",
        "            y = y[:, TARGET_IDX]\n",
        "        else:\n",
        "            # Fallback: just pick first column\n",
        "            y = y[:, 0]\n",
        "    else:\n",
        "        y = y.view(-1)\n",
        "    return y  # in eV\n",
        "\n",
        "\n",
        "def _forward_dense(model, batch):\n",
        "    \"\"\"\n",
        "    Convert PyG's sparse batch to dense (B, N, ...) and call the model.\n",
        "\n",
        "    Assumes model.forward(z_dense, pos_dense, mask).\n",
        "\n",
        "    Returns:\n",
        "      pred      : (B,) prediction in eV\n",
        "      sig_motor : scalar tensor (0.0 if model does not return it)\n",
        "    \"\"\"\n",
        "    batch = batch  # already on device\n",
        "\n",
        "    z         = batch.z          # (total_nodes,)\n",
        "    pos       = batch.pos        # (total_nodes, 3)\n",
        "    batch_idx = batch.batch      # (total_nodes,)\n",
        "\n",
        "    # Sparse â†’ dense\n",
        "    pos_dense, mask = to_dense_batch(pos, batch_idx)  # (B, N, 3), (B, N)\n",
        "    z_dense,  _     = to_dense_batch(z,   batch_idx)  # (B, N),    (_)\n",
        "\n",
        "    # Call the model with the correct signature\n",
        "    out = model(z_dense, pos_dense, mask)\n",
        "\n",
        "    # Handle either (pred, sig_motor) or just pred\n",
        "    if isinstance(out, tuple) and len(out) == 2:\n",
        "        pred, sig_motor = out\n",
        "    else:\n",
        "        pred = out\n",
        "        sig_motor = torch.zeros((), device=pred.device)\n",
        "\n",
        "    pred = pred.view(-1)  # (B,)\n",
        "    return pred, sig_motor\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    motor_strength: float,\n",
        "    lambda_motor_reg: float = 0.0,\n",
        "    scale_to_meV: float = 1000.0,   # eV â†’ meV\n",
        "):\n",
        "    model.train()\n",
        "    total_mae = 0.0\n",
        "    total_mot = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        y = _extract_target(batch)  # (B,), in eV\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred, sig_motor = _forward_dense(model, batch)\n",
        "\n",
        "        if pred.numel() != y.numel():\n",
        "            raise RuntimeError(\n",
        "                f\"Shape mismatch in train_one_epoch: pred {pred.shape}, y {y.shape}\"\n",
        "            )\n",
        "\n",
        "        loss_data = F.l1_loss(pred, y)  # in eV\n",
        "        # Use motor_strength only as a weight on the regularizer\n",
        "        loss = loss_data + lambda_motor_reg * motor_strength * sig_motor\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mae_meV = loss_data.item() * scale_to_meV\n",
        "        total_mae += mae_meV\n",
        "        total_mot += sig_motor.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_mae / max(n_batches, 1), total_mot / max(n_batches, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    device,\n",
        "    motor_strength: float,\n",
        "    scale_to_meV: float = 1000.0,   # eV â†’ meV\n",
        "):\n",
        "    model.eval()\n",
        "    total_mae = 0.0\n",
        "    total_mot = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        y = _extract_target(batch)  # (B,), in eV\n",
        "\n",
        "        pred, sig_motor = _forward_dense(model, batch)\n",
        "\n",
        "        if pred.numel() != y.numel():\n",
        "            raise RuntimeError(\n",
        "                f\"Shape mismatch in eval_epoch: pred {pred.shape}, y {y.shape}\"\n",
        "            )\n",
        "\n",
        "        loss_data = F.l1_loss(pred, y)  # in eV\n",
        "        mae_meV = loss_data.item() * scale_to_meV\n",
        "\n",
        "        total_mae += mae_meV\n",
        "        total_mot += sig_motor.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_mae / max(n_batches, 1), total_mot / max(n_batches, 1)\n",
        "\n",
        "\n",
        "def run_curriculum_training(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    device,\n",
        "    epochs=200,\n",
        "    motor_unlock_meV=10.0,      # when train MAE < this (meV), start ramp\n",
        "    motor_ramp_epochs=50,\n",
        "    motor_max_strength=6.0,\n",
        "    lambda_motor_reg=1e-3,      # volume penalty weight\n",
        "    scale_to_meV=1000.0,        # U0 in eV â†’ log in meV\n",
        "    run_name=\"RB3m_curriculum_U0\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Curriculum training for U0:\n",
        "\n",
        "      - uses TARGET_IDX = 7 (U0) from QM9 (PyG, in eV)\n",
        "      - converts sparse (z, pos, batch_idx) â†’ dense (B, N, ...) inside\n",
        "      - logs all losses in meV with 3 decimals\n",
        "      - saves config.json, metrics.json, RB3m_last.pt, RB3m_best.pt\n",
        "        under /content/drive/MyDrive/GAHEAD_runs/{run_name}_TIMESTAMP\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "\n",
        "    # --- seeding for reproducibility-ish ---\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # --- unique run directory on Drive ---\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_dir = f\"/content/drive/MyDrive/GAHEAD_runs/{run_name}_{ts}\"\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # --- git SHA of QM9_project (if available) ---\n",
        "    git_sha = None\n",
        "    try:\n",
        "        git_sha = subprocess.check_output(\n",
        "            [\"git\", \"-C\", \"/content/QM9_project\", \"rev-parse\", \"HEAD\"],\n",
        "            text=True,\n",
        "        ).strip()\n",
        "    except Exception:\n",
        "        git_sha = None\n",
        "\n",
        "    print(\"Ep  |  mS | motors | train [meV] |  val [meV] | test MAE [meV] | best\")\n",
        "    print(\"---+-----+--------+-------------+------------+-----------------+------\")\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_epoch = -1\n",
        "    curriculum_unlocked = False\n",
        "    unlock_epoch = None\n",
        "    history = []\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        # ---- motor_strength schedule (curriculum) ----\n",
        "        if not curriculum_unlocked:\n",
        "            motor_strength = 0.0\n",
        "        else:\n",
        "            t = max(0, ep - unlock_epoch)\n",
        "            frac = min(1.0, t / max(motor_ramp_epochs, 1))\n",
        "            motor_strength = motor_max_strength * frac\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # ---- one full epoch ----\n",
        "        train_mae, train_mot = train_one_epoch(\n",
        "            model, train_loader, optimizer, device,\n",
        "            motor_strength=motor_strength,\n",
        "            lambda_motor_reg=lambda_motor_reg,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        val_mae,   val_mot   = eval_epoch(\n",
        "            model, val_loader, device,\n",
        "            motor_strength=motor_strength,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        test_mae,  test_mot  = eval_epoch(\n",
        "            model, test_loader, device,\n",
        "            motor_strength=motor_strength,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        # ---- unlock motors once scalar fit is good enough ----\n",
        "        if (not curriculum_unlocked) and (train_mae <= motor_unlock_meV):\n",
        "            curriculum_unlocked = True\n",
        "            unlock_epoch = ep\n",
        "\n",
        "        # ---- track best (by val mae) ----\n",
        "        is_best = val_mae < best_val\n",
        "        if is_best:\n",
        "            best_val = val_mae\n",
        "            best_epoch = ep\n",
        "        star = \"â­\" if is_best else \" \"\n",
        "\n",
        "        # ---- pretty log line ----\n",
        "        print(\n",
        "            f\"{ep:3d} | {motor_strength:4.2f} | {val_mot:6.3f} | \"\n",
        "            f\"{train_mae:11.3f} | {val_mae:10.3f} | {test_mae:15.3f} | {star}\"\n",
        "        )\n",
        "\n",
        "        # ---- save last & best weights ----\n",
        "        torch.save(model.state_dict(), os.path.join(run_dir, \"RB3m_last.pt\"))\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), os.path.join(run_dir, \"RB3m_best.pt\"))\n",
        "\n",
        "        # ---- accumulate history ----\n",
        "        history.append({\n",
        "            \"epoch\": ep,\n",
        "            \"train_mae_meV\": train_mae,\n",
        "            \"val_mae_meV\": val_mae,\n",
        "            \"test_mae_meV\": test_mae,\n",
        "            \"train_mot\": train_mot,\n",
        "            \"val_mot\": val_mot,\n",
        "            \"test_mot\": test_mot,\n",
        "            \"motor_strength\": motor_strength,\n",
        "            \"sec\": dt,\n",
        "        })\n",
        "\n",
        "    # ---- write metrics + config to JSON ----\n",
        "    metrics_path = os.path.join(run_dir, \"metrics.json\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "    config = {\n",
        "        \"run_name\": run_name,\n",
        "        \"timestamp\": ts,\n",
        "        \"git_sha_QM9_project\": git_sha,\n",
        "        \"TARGET_IDX\": TARGET_IDX,\n",
        "        \"epochs\": epochs,\n",
        "        \"motor_unlock_meV\": motor_unlock_meV,\n",
        "        \"motor_ramp_epochs\": motor_ramp_epochs,\n",
        "        \"motor_max_strength\": motor_max_strength,\n",
        "        \"lambda_motor_reg\": lambda_motor_reg,\n",
        "        \"scale_to_meV\": scale_to_meV,\n",
        "        \"seed\": seed,\n",
        "        \"model_class\": model.__class__.__name__,\n",
        "        \"trainable_params\": sum(p.numel() for p in model.parameters()\n",
        "                                if p.requires_grad),\n",
        "    }\n",
        "    config_path = os.path.join(run_dir, \"config.json\")\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"\\nBest val: {best_val:.3f} meV at epoch {best_epoch}\")\n",
        "    print(f\"Run directory: {run_dir}\")\n",
        "\n",
        "    return run_dir, history\n"
      ],
      "metadata": {
        "id": "d2rj6v0Kw1_R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL X - Make sure QM9_project is on the path and import the model\n",
        "\n",
        "import os, sys\n",
        "\n",
        "# 1) Go to the repo root (adjust if your clone lives somewhere else)\n",
        "if os.path.isdir(\"/content/QM9_project\"):\n",
        "    os.chdir(\"/content/QM9_project\")\n",
        "else:\n",
        "    # Fallback: clone if needed\n",
        "    !git clone https://github.com/zeugirdoR/QM9_project.git /content/QM9_project\n",
        "    os.chdir(\"/content/QM9_project\")\n",
        "\n",
        "# 2) Ensure repo root is on sys.path\n",
        "if \"/content/QM9_project\" not in sys.path:\n",
        "    sys.path.insert(0, \"/content/QM9_project\")\n",
        "\n",
        "print(\"CWD:\", os.getcwd())\n",
        "print(\"sys.path[0]:\", sys.path[0])\n",
        "\n",
        "# 3) Now import the model\n",
        "import importlib\n",
        "import models.v20_agaa_micro as v20\n",
        "importlib.reload(v20)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = v20.V20_AGAA_Motor(\n",
        "    num_layers=16,\n",
        "    d_model=384,\n",
        "    n_heads=16,\n",
        "    max_z=100,\n",
        "    n_rbf=32,\n",
        ").to(device)\n",
        "\n",
        "print(\"Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIwK2p01tnQV",
        "outputId": "afadae0e-ed91-4bb8-d14b-33ab10563535"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /content/QM9_project\n",
            "sys.path[0]: /content/QM9_project\n",
            "Trainable params: 2714497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6 - Define the 2.7Mâ€“2.9M-parameter V20-AGAA-Motor and launch curriculum\n",
        "\n",
        "import importlib\n",
        "import models.v20_agaa_micro as v20\n",
        "importlib.reload(v20)\n",
        "\n",
        "# This is the \"big brain\" version; adjust if you want EXACTLY the 2.9M config\n",
        "model = v20.V20_AGAA_Motor(\n",
        "    num_layers=16,\n",
        "    d_model=384,\n",
        "    n_heads=16,\n",
        "    max_z=100,\n",
        "    n_rbf=32,\n",
        ").to(device)\n",
        "\n",
        "print(\"Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "RUN_DIR, history = run_curriculum_training(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    epochs=200,\n",
        "    motor_unlock_meV=10.0,      # when scalar-only fit reaches ~0.01 eV (~10 meV)\n",
        "    motor_ramp_epochs=50,\n",
        "    motor_max_strength=6.0,\n",
        "    lambda_motor_reg=1e-3,\n",
        "    scale_to_meV=1000.0,        # *** key: logs in meV ***\n",
        "    run_name=\"RB3m_curriculum_U0\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktIjKjDHtN9Z",
        "outputId": "84de686c-5536-4026-e57f-9647d2c7dc44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 2714497\n",
            "Ep  |  mS | motors | train [meV] |  val [meV] | test MAE [meV] | best\n",
            "---+-----+--------+-------------+------------+-----------------+------\n",
            "  1 | 0.00 | 1300.858 | 11036583.171 | 11493099.985 |    12423768.494 | â­\n",
            "  2 | 0.00 | 1299.717 | 11013693.886 | 11467566.669 |    12398235.168 | â­\n",
            "  3 | 0.00 | 1298.581 | 10985552.167 | 11436647.761 |    12367316.101 | â­\n",
            "  4 | 0.00 | 1297.453 | 10951889.090 | 11400456.355 |    12331124.756 | â­\n",
            "  5 | 0.00 | 1296.335 | 10913118.859 | 11359013.033 |    12289681.641 | â­\n",
            "  6 | 0.00 | 1295.218 | 10869171.280 | 11312391.977 |    12243060.669 | â­\n",
            "  7 | 0.00 | 1294.099 | 10820033.212 | 11260749.324 |    12191417.664 | â­\n",
            "  8 | 0.00 | 1292.977 | 10765874.251 | 11204290.415 |    12134958.984 | â­\n",
            "  9 | 0.00 | 1291.855 | 10707033.335 | 11143245.455 |    12073913.879 | â­\n",
            " 10 | 0.00 | 1290.737 | 10643990.807 | 11077796.161 |    12008464.539 | â­\n",
            " 11 | 0.00 | 1289.619 | 10576253.161 | 11008162.447 |    11938830.933 | â­\n",
            " 12 | 0.00 | 1288.499 | 10504641.656 | 10934504.545 |    11865172.852 | â­\n",
            " 13 | 0.00 | 1287.380 | 10429272.379 | 10856956.318 |    11787624.756 | â­\n",
            " 14 | 0.00 | 1286.263 | 10349617.165 | 10775656.588 |    11706325.073 | â­\n",
            " 15 | 0.00 | 1285.146 | 10266600.609 | 10690713.266 |    11621381.592 | â­\n",
            " 16 | 0.00 | 1284.029 | 10179856.077 | 10602231.783 |    11532900.330 | â­\n",
            " 17 | 0.00 | 1282.914 | 10089596.998 | 10510264.949 |    11440933.289 | â­\n",
            " 18 | 0.00 | 1281.800 | 9995842.178 | 10414914.438 |    11345582.886 | â­\n",
            " 19 | 0.00 | 1280.687 | 9898838.699 | 10316239.145 |    11246907.349 | â­\n",
            " 20 | 0.00 | 1279.575 | 9798759.402 | 10214292.255 |    11144960.876 | â­\n",
            " 21 | 0.00 | 1278.463 | 9694965.239 | 10109118.540 |    11039787.109 | â­\n",
            " 22 | 0.00 | 1277.354 | 9588224.491 | 10000766.977 |    10931435.486 | â­\n",
            " 23 | 0.00 | 1276.243 | 9478467.460 | 9889271.409 |    10819940.186 | â­\n",
            " 24 | 0.00 | 1275.132 | 9365321.021 | 9774667.255 |    10705335.632 | â­\n",
            " 25 | 0.00 | 1274.022 | 9249200.740 | 9656976.901 |    10587645.447 | â­\n",
            " 26 | 0.00 | 1272.911 | 9130071.957 | 9536235.652 |    10466904.358 | â­\n",
            " 27 | 0.00 | 1271.800 | 9007814.689 | 9412460.224 |    10343128.723 | â­\n",
            " 28 | 0.00 | 1270.692 | 8882435.751 | 9285680.026 |    10216348.511 | â­\n",
            " 29 | 0.00 | 1269.583 | 8754365.303 | 9155904.635 |    10086572.998 | â­\n",
            " 30 | 0.00 | 1268.476 | 8623078.293 | 9023148.700 |     9953817.261 | â­\n",
            " 31 | 0.00 | 1267.371 | 8488948.737 | 8887445.275 |     9818113.831 | â­\n",
            " 32 | 0.00 | 1266.265 | 8351798.992 | 8748787.447 |     9679455.872 | â­\n",
            " 33 | 0.00 | 1265.160 | 8211420.569 | 8607198.468 |     9537866.943 | â­\n",
            " 34 | 0.00 | 1264.057 | 8068317.905 | 8462690.805 |     9393359.436 | â­\n",
            " 35 | 0.00 | 1262.953 | 7922353.216 | 8315274.395 |     9245942.444 | â­\n",
            " 36 | 0.00 | 1261.852 | 7773649.693 | 8164972.187 |     9095640.869 | â­\n",
            " 37 | 0.00 | 1260.753 | 7622054.297 | 8011785.494 |     8942453.949 | â­\n",
            " 38 | 0.00 | 1259.654 | 7467431.243 | 7855714.299 |     8786382.721 | â­\n",
            " 39 | 0.00 | 1258.556 | 7309969.717 | 7696786.433 |     8627454.987 | â­\n",
            " 40 | 0.00 | 1257.459 | 7149463.454 | 7534987.849 |     8465656.403 | â­\n",
            " 41 | 0.00 | 1256.363 | 6986341.177 | 7370350.304 |     8301018.677 | â­\n",
            " 42 | 0.00 | 1255.266 | 6820519.851 | 7202858.436 |     8133526.794 | â­\n",
            " 43 | 0.00 | 1254.164 | 6651723.571 | 7032587.590 |     7963255.890 | â­\n",
            " 44 | 0.00 | 1253.069 | 6479951.526 | 6859473.445 |     7790141.815 | â­\n",
            " 45 | 0.00 | 1251.975 | 6305465.668 | 6683537.053 |     7614205.322 | â­\n",
            " 46 | 0.00 | 1250.883 | 6128258.591 | 6504852.746 |     7435521.240 | â­\n",
            " 47 | 0.00 | 1249.790 | 5948142.987 | 6323254.845 |     7253923.157 | â­\n",
            " 48 | 0.00 | 1248.700 | 5765345.085 | 6138966.665 |     7069634.949 | â­\n",
            " 49 | 0.00 | 1247.610 | 5580004.335 | 5951928.016 |     6882596.558 | â­\n",
            " 50 | 0.00 | 1246.523 | 5391741.781 | 5762053.748 |     6692722.015 | â­\n",
            " 51 | 0.00 | 1245.436 | 5200828.754 | 5569488.601 |     6500157.166 | â­\n",
            " 52 | 0.00 | 1244.351 | 5006911.176 | 5374090.520 |     6304758.911 | â­\n",
            " 53 | 0.00 | 1243.268 | 4810458.042 | 5175983.530 |     6106651.672 | â­\n",
            " 54 | 0.00 | 1242.184 | 4611724.276 | 4975391.489 |     5906059.753 | â­\n",
            " 55 | 0.00 | 1241.101 | 4410195.387 | 4771993.239 |     5702661.652 | â­\n",
            " 56 | 0.00 | 1240.019 | 4206400.015 | 4566028.677 |     5496697.113 | â­\n",
            " 57 | 0.00 | 1238.939 | 3999989.221 | 4357229.567 |     5287897.888 | â­\n",
            " 58 | 0.00 | 1237.859 | 3790811.789 | 4145792.809 |     5076461.197 | â­\n",
            " 59 | 0.00 | 1236.781 | 3579825.672 | 3932532.565 |     4863200.989 | â­\n",
            " 60 | 0.00 | 1235.702 | 3367873.517 | 3716627.319 |     4647295.776 | â­\n",
            " 61 | 0.00 | 1234.626 | 3153827.711 | 3498625.188 |     4429293.823 | â­\n",
            " 62 | 0.00 | 1233.552 | 2937953.893 | 3277769.822 |     4208438.248 | â­\n",
            " 63 | 0.00 | 1232.476 | 2720286.630 | 3055019.419 |     3985687.729 | â­\n",
            " 64 | 0.00 | 1231.404 | 2506075.081 | 2833908.119 |     3764576.599 | â­\n",
            " 65 | 0.00 | 1230.315 | 2294507.995 | 2610585.299 |     3541253.723 | â­\n",
            " 66 | 0.00 | 1229.244 | 2085586.985 | 2387681.988 |     3318350.372 | â­\n",
            " 67 | 0.00 | 1228.175 | 1877663.145 | 2161336.078 |     3092004.463 | â­\n",
            " 68 | 0.00 | 1227.107 | 1674705.164 | 1943859.755 |     2874219.658 | â­\n",
            " 69 | 0.00 | 1226.039 | 1503452.004 | 1749243.634 |     2670525.551 | â­\n",
            " 70 | 0.00 | 1224.971 | 1344624.578 | 1558730.192 |     2470399.712 | â­\n",
            " 71 | 0.00 | 1223.906 | 1212612.526 | 1387243.488 |     2286460.960 | â­\n",
            " 72 | 0.00 | 1222.843 | 1089085.350 | 1209851.126 |     2095530.190 | â­\n",
            " 73 | 0.00 | 1221.780 |  977596.353 | 1060283.424 |     1934148.590 | â­\n",
            " 74 | 0.00 | 1220.718 |  918482.740 | 983366.636 |     1831541.336 | â­\n",
            " 75 | 0.00 | 1219.657 |  886800.837 | 916803.385 |     1729126.362 | â­\n",
            " 76 | 0.00 | 1218.596 |  854862.789 | 850657.135 |     1627101.280 | â­\n",
            " 77 | 0.00 | 1217.537 |  828894.285 | 801472.961 |     1552099.327 | â­\n",
            " 78 | 0.00 | 1216.428 |  820273.271 | 787221.609 |     1529707.268 | â­\n",
            " 79 | 0.00 | 1215.197 |  819416.207 | 786839.388 |     1529037.308 | â­\n",
            " 80 | 0.00 | 1214.137 |  819485.781 | 787238.705 |     1529736.374 |  \n",
            " 81 | 0.00 | 1213.088 |  819478.726 | 786776.419 |     1528924.225 | â­\n",
            " 82 | 0.00 | 1212.032 |  819463.973 | 786600.173 |     1528606.201 | â­\n",
            " 83 | 0.00 | 1210.974 |  819359.146 | 786643.253 |     1528683.979 |  \n",
            " 84 | 0.00 | 1209.923 |  819323.966 | 787346.026 |     1529918.785 |  \n",
            " 85 | 0.00 | 1208.870 |  819500.872 | 787054.603 |     1529419.796 |  \n",
            " 86 | 0.00 | 1207.851 |  819443.504 | 786788.572 |     1528946.144 |  \n",
            " 87 | 0.00 | 1206.816 |  819407.590 | 787704.944 |     1530514.961 |  \n",
            " 88 | 0.00 | 1205.755 |  819339.045 | 786861.149 |     1529076.347 |  \n",
            " 89 | 0.00 | 1204.720 |  819299.103 | 786923.073 |     1529187.160 |  \n",
            " 90 | 0.00 | 1203.616 |  819326.837 | 786566.810 |     1528545.776 | â­\n",
            " 91 | 0.00 | 1202.565 |  819411.601 | 787212.496 |     1529691.727 |  \n",
            " 92 | 0.00 | 1201.524 |  819404.691 | 786398.622 |     1528240.112 | â­\n",
            " 93 | 0.00 | 1200.384 |  819454.397 | 786215.417 |     1527905.136 | â­\n",
            " 94 | 0.00 | 1199.398 |  819307.422 | 785348.702 |     1526315.434 | â­\n",
            " 95 | 0.00 | 1198.335 |  819375.199 | 787197.666 |     1529666.439 |  \n",
            " 96 | 0.00 | 1197.285 |  819402.676 | 787316.567 |     1529868.965 |  \n",
            " 97 | 0.00 | 1196.234 |  819353.931 | 785216.246 |     1526072.514 | â­\n",
            " 98 | 0.00 | 1195.191 |  819323.496 | 786720.859 |     1528824.242 |  \n",
            " 99 | 0.00 | 1194.144 |  819376.258 | 786519.501 |     1528459.927 |  \n",
            "100 | 0.00 | 1193.101 |  819512.550 | 786666.352 |     1528725.967 |  \n",
            "101 | 0.00 | 1192.066 |  819354.108 | 785547.867 |     1526680.748 |  \n",
            "102 | 0.00 | 1191.015 |  819405.325 | 787214.424 |     1529695.026 |  \n",
            "103 | 0.00 | 1189.976 |  819439.791 | 786471.392 |     1528372.536 |  \n",
            "104 | 0.00 | 1188.940 |  819372.669 | 783723.963 |     1523335.449 | â­\n",
            "105 | 0.00 | 1187.901 |  819380.384 | 786475.194 |     1528379.517 |  \n",
            "106 | 0.00 | 1186.837 |  819410.011 | 786411.267 |     1528263.237 |  \n",
            "107 | 0.00 | 1185.800 |  819424.391 | 783914.869 |     1523685.631 |  \n",
            "108 | 0.00 | 1184.761 |  819526.582 | 787471.775 |     1530129.864 |  \n",
            "109 | 0.00 | 1183.738 |  819377.247 | 786135.754 |     1527759.022 |  \n",
            "110 | 0.00 | 1182.710 |  819439.625 | 786116.819 |     1527724.125 |  \n",
            "111 | 0.00 | 1181.681 |  819397.188 | 785154.674 |     1525959.549 |  \n",
            "112 | 0.00 | 1180.651 |  819491.617 | 785747.974 |     1527047.737 |  \n",
            "113 | 0.00 | 1179.619 |  819471.934 | 786896.686 |     1529140.068 |  \n",
            "114 | 0.00 | 1178.592 |  819349.217 | 786964.760 |     1529261.372 |  \n",
            "115 | 0.00 | 1177.564 |  819375.486 | 786590.438 |     1528588.470 |  \n",
            "116 | 0.00 | 1176.537 |  819387.703 | 787361.100 |     1529944.286 |  \n",
            "117 | 0.00 | 1175.507 |  819389.967 | 785701.302 |     1526962.135 |  \n",
            "118 | 0.00 | 1174.482 |  819393.580 | 786989.098 |     1529304.573 |  \n",
            "119 | 0.00 | 1173.457 |  819411.587 | 786332.881 |     1528120.022 |  \n",
            "120 | 0.00 | 1172.435 |  819304.079 | 785229.807 |     1526097.351 |  \n",
            "121 | 0.00 | 1171.420 |  819451.439 | 785342.506 |     1526304.058 |  \n",
            "122 | 0.00 | 1170.395 |  819353.213 | 786986.464 |     1529299.740 |  \n",
            "123 | 0.00 | 1169.393 |  819421.485 | 785729.218 |     1527013.290 |  \n",
            "124 | 0.00 | 1168.372 |  819449.674 | 785509.058 |     1526609.524 |  \n",
            "125 | 0.00 | 1167.352 |  819419.702 | 786491.011 |     1528408.066 |  \n",
            "126 | 0.00 | 1166.333 |  819295.605 | 784986.789 |     1525651.653 |  \n",
            "127 | 0.00 | 1165.314 |  819476.999 | 786845.277 |     1529047.924 |  \n",
            "128 | 0.00 | 1164.298 |  819382.229 | 787410.521 |     1530027.439 |  \n",
            "129 | 0.00 | 1163.285 |  819449.033 | 785955.570 |     1527428.463 |  \n",
            "130 | 0.00 | 1162.269 |  819299.646 | 786816.709 |     1528996.632 |  \n",
            "131 | 0.00 | 1161.253 |  819400.810 | 784990.775 |     1525658.890 |  \n",
            "132 | 0.00 | 1160.242 |  819296.023 | 786373.289 |     1528193.920 |  \n",
            "133 | 0.00 | 1159.231 |  819435.742 | 786360.295 |     1528170.238 |  \n",
            "134 | 0.00 | 1158.221 |  819432.781 | 787081.370 |     1529466.263 |  \n",
            "135 | 0.00 | 1157.210 |  819446.507 | 786420.450 |     1528279.842 |  \n",
            "136 | 0.00 | 1156.202 |  819508.411 | 785879.855 |     1527289.719 |  \n",
            "137 | 0.00 | 1155.196 |  819286.512 | 787095.501 |     1529490.818 |  \n",
            "138 | 0.00 | 1154.189 |  819355.103 | 784994.766 |     1525666.260 |  \n",
            "139 | 0.00 | 1153.184 |  819355.401 | 786569.386 |     1528550.472 |  \n",
            "140 | 0.00 | 1152.176 |  819275.344 | 788114.417 |     1531181.473 |  \n",
            "141 | 0.00 | 1151.171 |  819337.956 | 784555.197 |     1524859.829 |  \n",
            "142 | 0.00 | 1150.167 |  819350.747 | 785510.861 |     1526612.888 |  \n",
            "143 | 0.00 | 1149.164 |  819449.858 | 786888.098 |     1529124.454 |  \n",
            "144 | 0.00 | 1148.163 |  819461.266 | 785217.131 |     1526074.142 |  \n",
            "145 | 0.00 | 1147.163 |  819434.558 | 787015.961 |     1529351.994 |  \n",
            "146 | 0.00 | 1146.164 |  819472.990 | 787516.261 |     1530203.869 |  \n",
            "147 | 0.00 | 1145.165 |  819321.948 | 786716.764 |     1528816.883 |  \n",
            "148 | 0.00 | 1144.167 |  819396.179 | 786893.446 |     1529134.186 |  \n",
            "149 | 0.00 | 1143.171 |  819344.155 | 785860.785 |     1527254.623 |  \n",
            "150 | 0.00 | 1142.176 |  819420.404 | 784995.317 |     1525667.370 |  \n",
            "151 | 0.00 | 1141.181 |  819340.413 | 786631.805 |     1528663.456 |  \n",
            "152 | 0.00 | 1140.188 |  819450.439 | 786355.399 |     1528161.327 |  \n",
            "153 | 0.00 | 1139.196 |  819361.109 | 784843.714 |     1525389.221 |  \n",
            "154 | 0.00 | 1138.205 |  819438.198 | 786771.501 |     1528915.264 |  \n",
            "155 | 0.00 | 1137.215 |  819525.222 | 785180.374 |     1526006.699 |  \n",
            "156 | 0.00 | 1136.226 |  819247.713 | 786016.753 |     1527540.638 |  \n",
            "157 | 0.00 | 1135.237 |  819374.267 | 785868.926 |     1527269.493 |  \n",
            "158 | 0.00 | 1134.251 |  819405.888 | 787052.805 |     1529416.767 |  \n",
            "159 | 0.00 | 1133.264 |  819408.724 | 786605.723 |     1528616.280 |  \n",
            "160 | 0.00 | 1132.279 |  819330.722 | 787135.729 |     1529560.268 |  \n",
            "161 | 0.00 | 1131.294 |  819355.391 | 786098.211 |     1527690.086 |  \n",
            "162 | 0.00 | 1130.309 |  819301.488 | 786739.293 |     1528857.254 |  \n",
            "163 | 0.00 | 1129.326 |  819398.809 | 786174.411 |     1527829.803 |  \n",
            "164 | 0.00 | 1128.343 |  819328.371 | 785549.357 |     1526683.468 |  \n",
            "165 | 0.00 | 1127.361 |  819470.006 | 786181.940 |     1527843.689 |  \n",
            "166 | 0.00 | 1126.381 |  819395.565 | 787016.090 |     1529352.310 |  \n",
            "167 | 0.00 | 1125.402 |  819391.014 | 786098.479 |     1527690.395 |  \n",
            "168 | 0.00 | 1124.423 |  819450.111 | 784268.362 |     1524333.931 |  \n",
            "169 | 0.00 | 1123.445 |  819579.940 | 787483.979 |     1530150.219 |  \n",
            "170 | 0.00 | 1122.467 |  819301.796 | 786347.390 |     1528146.675 |  \n",
            "171 | 0.00 | 1121.491 |  819349.537 | 786728.558 |     1528838.184 |  \n",
            "172 | 0.00 | 1120.515 |  819389.155 | 785745.622 |     1527043.457 |  \n",
            "173 | 0.00 | 1119.540 |  819295.150 | 786373.642 |     1528194.580 |  \n",
            "174 | 0.00 | 1118.569 |  819398.397 | 785849.194 |     1527233.295 |  \n",
            "175 | 0.00 | 1117.596 |  819378.671 | 785877.612 |     1527285.488 |  \n",
            "176 | 0.00 | 1116.626 |  819519.634 | 788038.965 |     1531060.173 |  \n",
            "177 | 0.00 | 1115.655 |  819285.050 | 786419.039 |     1528277.187 |  \n",
            "178 | 0.00 | 1114.685 |  819471.678 | 786319.389 |     1528095.501 |  \n",
            "179 | 0.00 | 1113.716 |  819309.741 | 784503.508 |     1524765.259 |  \n",
            "180 | 0.00 | 1112.747 |  819355.143 | 784306.710 |     1524404.423 |  \n",
            "181 | 0.00 | 1111.780 |  819366.618 | 784740.494 |     1525199.898 |  \n",
            "182 | 0.00 | 1110.813 |  819497.286 | 785263.200 |     1526158.550 |  \n",
            "183 | 0.00 | 1109.847 |  819382.899 | 785211.299 |     1526063.385 |  \n",
            "184 | 0.00 | 1108.882 |  819384.353 | 786733.917 |     1528847.576 |  \n",
            "185 | 0.00 | 1107.918 |  819365.359 | 787122.926 |     1529538.189 |  \n",
            "186 | 0.00 | 1106.955 |  819450.690 | 784403.387 |     1524581.619 |  \n",
            "187 | 0.00 | 1105.994 |  819391.937 | 786741.621 |     1528861.622 |  \n",
            "188 | 0.00 | 1105.032 |  819418.264 | 785730.234 |     1527015.488 |  \n",
            "189 | 0.00 | 1104.069 |  819358.834 | 784679.923 |     1525088.959 |  \n",
            "190 | 0.00 | 1103.108 |  819394.124 | 786465.587 |     1528361.984 |  \n",
            "191 | 0.00 | 1102.147 |  819382.302 | 785066.085 |     1525797.066 |  \n",
            "192 | 0.00 | 1101.187 |  819419.770 | 783819.983 |     1523511.684 |  \n",
            "193 | 0.00 | 1100.229 |  819434.681 | 786578.404 |     1528566.914 |  \n",
            "194 | 0.00 | 1099.271 |  819371.926 | 785253.061 |     1526139.938 |  \n",
            "195 | 0.00 | 1098.313 |  819358.084 | 786312.965 |     1528083.897 |  \n",
            "196 | 0.00 | 1097.356 |  819386.362 | 785728.952 |     1527013.000 |  \n",
            "197 | 0.00 | 1096.400 |  819359.085 | 787239.764 |     1529738.300 |  \n",
            "198 | 0.00 | 1095.445 |  819394.185 | 786413.784 |     1528267.723 |  \n",
            "199 | 0.00 | 1094.490 |  819309.236 | 785409.831 |     1526427.532 |  \n",
            "200 | 0.00 | 1093.536 |  819402.014 | 786813.086 |     1528990.036 |  \n",
            "\n",
            "Best val: 783723.963 meV at epoch 104\n",
            "Run directory: /content/drive/MyDrive/GAHEAD_runs/RB3m_curriculum_U0_20251129_131927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CELL: U0 curriculum training utilities (reproducible, meV logging) ====\n",
        "#### This overrides previous helpers....\n",
        "\n",
        "import os, json, time, math, random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- U0 target index in PyG QM9 ---\n",
        "TARGET_IDX = 7   # confirmed U0 column\n",
        "\n",
        "# If you want to fix seeds for reproducibility:\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "def _extract_target(batch):\n",
        "    \"\"\"\n",
        "    Extract U0 (index 7) as a 1D tensor in eV.\n",
        "    Handles shapes (B,19), (B,1), (B,).\n",
        "    \"\"\"\n",
        "    y = batch.y\n",
        "    if y.dim() == 2:\n",
        "        if y.size(1) >= TARGET_IDX + 1:\n",
        "            y = y[:, TARGET_IDX]\n",
        "        else:\n",
        "            y = y[:, 0]\n",
        "    else:\n",
        "        y = y.view(-1)\n",
        "    return y  # in eV\n",
        "\n",
        "\n",
        "def _call_model_with_optional_motor(model, z, pos, batch_idx, motor_strength):\n",
        "    \"\"\"\n",
        "    Call V20_AGAA_Motor robustly:\n",
        "      - If it accepts motor_strength, use it.\n",
        "      - Otherwise, call without it.\n",
        "    Returns (pred, sig_scalar) where:\n",
        "      - pred: (B,) or (B,1) in eV\n",
        "      - sig_scalar: scalar summary (float) of any \"motor\" / geometric activity\n",
        "    \"\"\"\n",
        "    # Try the \"new\" signature with motor_strength\n",
        "    try:\n",
        "        out = model(z, pos, batch_idx, motor_strength=motor_strength)\n",
        "    except TypeError:\n",
        "        # Fallback: old signature without motor_strength\n",
        "        out = model(z, pos, batch_idx)\n",
        "\n",
        "    # Unpack (pred, sig) or just pred\n",
        "    if isinstance(out, tuple) and len(out) == 2:\n",
        "        pred, sig = out\n",
        "        # reduce sig to scalar\n",
        "        if torch.is_tensor(sig):\n",
        "            sig_scalar = float(sig.mean().detach().cpu())\n",
        "        else:\n",
        "            sig_scalar = float(sig)\n",
        "    else:\n",
        "        pred = out\n",
        "        sig_scalar = 0.0\n",
        "\n",
        "    # Flatten prediction to (B,)\n",
        "    pred = pred.view(-1)\n",
        "    return pred, sig_scalar\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    motor_strength: float,\n",
        "    lambda_motor_reg: float = 0.0,\n",
        "    scale_to_meV: float = 1000.0,  # multiply eV â†’ meV for logging\n",
        "):\n",
        "    model.train()\n",
        "    total_mae_meV = 0.0\n",
        "    total_mot = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        z         = batch.z          # (total_nodes,)\n",
        "        pos       = batch.pos        # (total_nodes, 3)\n",
        "        batch_idx = batch.batch      # (total_nodes,)\n",
        "        y_eV      = _extract_target(batch)  # (B,), in eV\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Robust call (with or without motor_strength support)\n",
        "        pred_eV, sig_scalar = _call_model_with_optional_motor(\n",
        "            model, z, pos, batch_idx, motor_strength\n",
        "        )\n",
        "\n",
        "        if pred_eV.numel() != y_eV.numel():\n",
        "            raise RuntimeError(\n",
        "                f\"Shape mismatch in train_one_epoch: pred {pred_eV.shape}, y {y_eV.shape}\"\n",
        "            )\n",
        "\n",
        "        # MAE in eV\n",
        "        loss_data = F.l1_loss(pred_eV, y_eV)\n",
        "\n",
        "        # Approximate \"volume\" penalty via scalar/motor activity\n",
        "        loss = loss_data + lambda_motor_reg * motor_strength * sig_scalar\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging in meV\n",
        "        mae_meV = loss_data.item() * scale_to_meV\n",
        "        total_mae_meV += mae_meV\n",
        "        total_mot += sig_scalar\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_mae_meV / max(n_batches, 1), total_mot / max(n_batches, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    device,\n",
        "    motor_strength: float,\n",
        "    scale_to_meV: float = 1000.0,\n",
        "):\n",
        "    model.eval()\n",
        "    total_mae_meV = 0.0\n",
        "    total_mot = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        z         = batch.z\n",
        "        pos       = batch.pos\n",
        "        batch_idx = batch.batch\n",
        "        y_eV      = _extract_target(batch)  # (B,)\n",
        "\n",
        "        pred_eV, sig_scalar = _call_model_with_optional_motor(\n",
        "            model, z, pos, batch_idx, motor_strength\n",
        "        )\n",
        "        pred_eV = pred_eV.view(-1)\n",
        "\n",
        "        if pred_eV.numel() != y_eV.numel():\n",
        "            raise RuntimeError(\n",
        "                f\"Shape mismatch in eval_epoch: pred {pred_eV.shape}, y {y_eV.shape}\"\n",
        "            )\n",
        "\n",
        "        loss_data = F.l1_loss(pred_eV, y_eV)      # MAE in eV\n",
        "        mae_meV = loss_data.item() * scale_to_meV # log in meV\n",
        "\n",
        "        total_mae_meV += mae_meV\n",
        "        total_mot += sig_scalar\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_mae_meV / max(n_batches, 1), total_mot / max(n_batches, 1)\n",
        "\n",
        "\n",
        "def run_curriculum_training(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    device,\n",
        "    epochs=200,\n",
        "    motor_unlock_meV=1000.0,    # ðŸ” looser unlock threshold (in meV)\n",
        "    motor_ramp_epochs=50,\n",
        "    motor_max_strength=6.0,\n",
        "    lambda_motor_reg=1e-3,\n",
        "    scale_to_meV=1000.0,        # U0 in eV â†’ log in meV\n",
        "    run_name=\"RB3m_curriculum_U0\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Curriculum training for U0:\n",
        "\n",
        "      - uses TARGET_IDX = 7 (U0) from QM9 (PyG)\n",
        "      - assumes model = V20_AGAA_Motor\n",
        "      - logs MAE in meV with 3 decimals\n",
        "      - saves config.json, metrics.json, and last/best .pt files\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "\n",
        "    # --- set up a unique run directory on Drive ---\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_dir = f\"/content/drive/MyDrive/GAHEAD_runs/{run_name}_{ts}\"\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # --- save a minimal config for reproducibility ---\n",
        "    cfg = {\n",
        "        \"run_name\": run_name,\n",
        "        \"timestamp\": ts,\n",
        "        \"target_idx\": TARGET_IDX,\n",
        "        \"scale_to_meV\": scale_to_meV,\n",
        "        \"motor_unlock_meV\": motor_unlock_meV,\n",
        "        \"motor_ramp_epochs\": motor_ramp_epochs,\n",
        "        \"motor_max_strength\": motor_max_strength,\n",
        "        \"lambda_motor_reg\": lambda_motor_reg,\n",
        "        \"optimizer\": \"AdamW\",\n",
        "        \"lr\": 2e-4,\n",
        "        \"model_params\": {\n",
        "            \"num_layers\": getattr(model, \"num_layers\", None),\n",
        "            \"d_model\":    getattr(model, \"d_model\", None),\n",
        "            \"n_heads\":    getattr(model, \"n_heads\", None),\n",
        "            \"max_z\":      getattr(model, \"max_z\", None),\n",
        "            \"n_rbf\":      getattr(model, \"n_rbf\", None),\n",
        "            \"param_count\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "        },\n",
        "        \"seed\": SEED,\n",
        "    }\n",
        "    with open(os.path.join(run_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(cfg, f, indent=2)\n",
        "\n",
        "    history = []\n",
        "    best_val = float(\"inf\")\n",
        "    best_epoch = -1\n",
        "    curriculum_unlocked = False\n",
        "    unlock_epoch = None\n",
        "\n",
        "    print(\"Ep  |  mS | motors | train [meV] |  val [meV] | test MAE [meV] | best\")\n",
        "    print(\"---+-----+--------+-------------+------------+-----------------+------\")\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        # decide motor_strength for this epoch\n",
        "        if not curriculum_unlocked:\n",
        "            motor_strength = 0.0\n",
        "        else:\n",
        "            t = max(0, ep - unlock_epoch)\n",
        "            motor_strength = motor_max_strength * min(1.0, t / max(motor_ramp_epochs, 1))\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # --- one full epoch ---\n",
        "        train_mae, train_mot = train_one_epoch(\n",
        "            model, train_loader, optimizer, device,\n",
        "            motor_strength=motor_strength,\n",
        "            lambda_motor_reg=lambda_motor_reg,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        val_mae,   val_mot   = eval_epoch(\n",
        "            model, val_loader, device,\n",
        "            motor_strength=motor_strength,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        test_mae,  test_mot  = eval_epoch(\n",
        "            model, test_loader, device,\n",
        "            motor_strength=motor_strength,\n",
        "            scale_to_meV=scale_to_meV,\n",
        "        )\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        # unlock motors when scalar-only fit is good enough\n",
        "        if (not curriculum_unlocked) and (train_mae <= motor_unlock_meV):\n",
        "            curriculum_unlocked = True\n",
        "            unlock_epoch = ep\n",
        "\n",
        "        # best mark based on val\n",
        "        is_best = val_mae < best_val\n",
        "        if is_best:\n",
        "            best_val = val_mae\n",
        "            best_epoch = ep\n",
        "\n",
        "            # Save best weights\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(run_dir, \"RB3m_curriculum_best.pt\"),\n",
        "            )\n",
        "\n",
        "        star = \"â­\" if is_best else \" \"\n",
        "\n",
        "        # store metrics\n",
        "        history.append({\n",
        "            \"epoch\": ep,\n",
        "            \"train_mae_meV\": train_mae,\n",
        "            \"val_mae_meV\":   val_mae,\n",
        "            \"test_mae_meV\":  test_mae,\n",
        "            \"motor_strength\": motor_strength,\n",
        "            \"train_mot\": train_mot,\n",
        "            \"val_mot\":   val_mot,\n",
        "            \"test_mot\":  test_mot,\n",
        "            \"sec\": dt,\n",
        "        })\n",
        "\n",
        "        # pretty log with 3 decimals on MAE\n",
        "        print(\n",
        "            f\"{ep:3d} | {motor_strength:4.2f} | {val_mot:8.3f} | \"\n",
        "            f\"{train_mae:13.3f} | {val_mae:10.3f} | {test_mae:15.3f} | {star}\"\n",
        "        )\n",
        "\n",
        "    # Save final (last) weights\n",
        "    torch.save(\n",
        "        model.state_dict(),\n",
        "        os.path.join(run_dir, \"RB3m_curriculum_last.pt\"),\n",
        "    )\n",
        "\n",
        "    # Save metrics\n",
        "    with open(os.path.join(run_dir, \"metrics.json\"), \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "    print(f\"\\nBest val: {best_val:.3f} meV at epoch {best_epoch}\")\n",
        "    print(\"Run directory:\", run_dir)\n",
        "    return run_dir, history\n"
      ],
      "metadata": {
        "id": "T7StNQFu9iF7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this single cell (it will override the old helper function; no need to touch the rest):\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "import torch\n",
        "\n",
        "def _call_model_with_optional_motor(model, z, pos, batch_idx, motor_strength):\n",
        "    \"\"\"\n",
        "    Robustly call V20_AGAA_Motor:\n",
        "\n",
        "      - If the model supports motor_strength in the forward signature, use it.\n",
        "      - Otherwise, assume the model expects dense (z_dense, pos_dense, mask)\n",
        "        and convert from the sparse (z, pos, batch_idx) representation.\n",
        "\n",
        "    Returns:\n",
        "      pred_eV:    (B,) tensor, predictions in eV\n",
        "      sig_scalar: float, scalar \"motors\" activity summary\n",
        "    \"\"\"\n",
        "    # 1) Try \"new\" signature: model(z, pos, batch_idx, motor_strength=...)\n",
        "    try:\n",
        "        out = model(z, pos, batch_idx, motor_strength=motor_strength)\n",
        "    except TypeError:\n",
        "        # 2) Fallback: model expects dense (z_dense, pos_dense, mask)\n",
        "        pos_dense, mask = to_dense_batch(pos, batch_idx)  # (B, N, 3), (B, N)\n",
        "        z_dense, _      = to_dense_batch(z,   batch_idx)  # (B, N)\n",
        "        out = model(z_dense, pos_dense, mask)\n",
        "\n",
        "    # 3) Unpack (pred, sig) or just pred\n",
        "    if isinstance(out, tuple) and len(out) == 2:\n",
        "        pred, sig = out\n",
        "        if torch.is_tensor(sig):\n",
        "            sig_scalar = float(sig.mean().detach().cpu())\n",
        "        else:\n",
        "            sig_scalar = float(sig)\n",
        "    else:\n",
        "        pred = out\n",
        "        sig_scalar = 0.0\n",
        "\n",
        "    # 4) Flatten pred to (B,)\n",
        "    pred = pred.view(-1)\n",
        "    return pred, sig_scalar\n"
      ],
      "metadata": {
        "id": "c_dBBu8dFWyJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CELL: launch U0 training with clean logging/saving ====\n",
        "\n",
        "print(\"Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "RUN_DIR, history = run_curriculum_training(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    epochs=200,\n",
        "    motor_unlock_meV=1000.0,   # start motors once we're < 1 eV MAE\n",
        "    motor_ramp_epochs=50,\n",
        "    motor_max_strength=6.0,\n",
        "    lambda_motor_reg=1e-3,\n",
        "    scale_to_meV=1000.0,\n",
        "    run_name=\"RB3m_curriculum_U0\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "CfI-WqoI-GQA",
        "outputId": "a0014577-c1d6-4bea-8784-efdb1355c7e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 2714497\n",
            "Ep  |  mS | motors | train [meV] |  val [meV] | test MAE [meV] | best\n",
            "---+-----+--------+-------------+------------+-----------------+------\n",
            "  1 | 0.00 | 1092.581 |    819401.620 | 785738.961 |     1527031.174 | â­\n",
            "  2 | 0.00 | 1091.629 |    819337.995 | 783960.664 |     1523769.650 | â­\n",
            "  3 | 0.00 | 1090.677 |    819509.002 | 785930.330 |     1527382.156 |  \n",
            "  4 | 0.00 | 1089.726 |    819398.941 | 786481.769 |     1528391.521 |  \n",
            "  5 | 0.00 | 1088.777 |    819392.765 | 787916.998 |     1530862.141 |  \n",
            "  6 | 0.00 | 1087.828 |    819453.895 | 786229.772 |     1527931.393 |  \n",
            "  7 | 0.00 | 1086.880 |    819314.370 | 786807.644 |     1528980.312 |  \n",
            "  8 | 0.00 | 1085.933 |    819307.709 | 785100.648 |     1525860.439 |  \n",
            "  9 | 0.00 | 1084.987 |    819365.521 | 786868.474 |     1529089.485 |  \n",
            " 10 | 0.00 | 1084.042 |    819447.254 | 785824.303 |     1527187.843 |  \n",
            " 11 | 0.00 | 1083.098 |    819500.181 | 787012.417 |     1529345.634 |  \n",
            " 12 | 0.00 | 1082.155 |    819350.177 | 786052.838 |     1527606.899 |  \n",
            " 13 | 0.00 | 1081.212 |    819428.788 | 785145.846 |     1525943.459 |  \n",
            " 14 | 0.00 | 1080.270 |    819465.795 | 787361.450 |     1529944.813 |  \n",
            " 15 | 0.00 | 1079.329 |    819451.128 | 786605.723 |     1528616.280 |  \n",
            " 16 | 0.00 | 1078.389 |    819469.108 | 786757.874 |     1528890.877 |  \n",
            " 17 | 0.00 | 1077.450 |    819319.490 | 786951.450 |     1529237.717 |  \n",
            " 18 | 0.00 | 1076.512 |    819315.630 | 786226.242 |     1527924.889 |  \n",
            " 19 | 0.00 | 1075.574 |    819434.727 | 785535.681 |     1526658.363 |  \n",
            " 20 | 0.00 | 1074.636 |    819294.758 | 785971.828 |     1527458.221 |  \n",
            " 21 | 0.00 | 1073.699 |    819402.297 | 784698.863 |     1525123.512 |  \n",
            " 22 | 0.00 | 1072.764 |    819200.942 | 786512.881 |     1528447.914 |  \n",
            " 23 | 0.00 | 1071.829 |    819341.411 | 786422.247 |     1528283.142 |  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3152355352.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainable params:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m RUN_DIR, history = run_curriculum_training(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1787964661.py\u001b[0m in \u001b[0;36mrun_curriculum_training\u001b[0;34m(model, train_loader, val_loader, test_loader, device, epochs, motor_unlock_meV, motor_ramp_epochs, motor_max_strength, lambda_motor_reg, scale_to_meV, run_name)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# --- one full epoch ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         train_mae, train_mot = train_one_epoch(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mmotor_strength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmotor_strength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1787964661.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, device, motor_strength, lambda_motor_reg, scale_to_meV)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Robust call (with or without motor_strength support)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         pred_eV, sig_scalar = _call_model_with_optional_motor(\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotor_strength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-326254046.py\u001b[0m in \u001b[0;36m_call_model_with_optional_motor\u001b[0;34m(model, z, pos, batch_idx, motor_strength)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# 1) Try \"new\" signature: model(z, pos, batch_idx, motor_strength=...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotor_strength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmotor_strength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 2) Fallback: model expects dense (z_dense, pos_dense, mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}